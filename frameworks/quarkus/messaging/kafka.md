# Kafka com Quarkus â€” Guia Completo de IntegraÃ§Ã£o e Microservices

## Ãndice

1. [O que Ã© Apache Kafka?](#1-o-que-Ã©-apache-kafka)
2. [Conceitos Fundamentais do Kafka](#2-conceitos-fundamentais-do-kafka)
3. [Kafka + Quarkus: A CombinaÃ§Ã£o Perfeita](#3-kafka--quarkus-a-combinaÃ§Ã£o-perfeita)
4. [Setup do Projeto Quarkus](#4-setup-do-projeto-quarkus)
5. [ConfiguraÃ§Ã£o do Kafka](#5-configuraÃ§Ã£o-do-kafka)
6. [Produzindo Mensagens](#6-produzindo-mensagens)
7. [Consumindo Mensagens](#7-consumindo-mensagens)
8. [Fluxos e Arquiteturas](#8-fluxos-e-arquiteturas)
9. [Boas PrÃ¡ticas](#9-boas-prÃ¡ticas)
10. [ConfiguraÃ§Ãµes AvanÃ§adas](#10-configuraÃ§Ãµes-avanÃ§adas)
11. [Testes e Qualidade](#11-testes-e-qualidade)
12. [ExercÃ­cios PrÃ¡ticos](#12-exercÃ­cios-prÃ¡ticos)
13. [Recursos e ReferÃªncias](#13-recursos-e-referÃªncias)

---

## 1. O que Ã© Apache Kafka?

### DefiniÃ§Ã£o

**Apache Kafka** Ã© uma plataforma de streaming de eventos distribuÃ­da e de cÃ³digo aberto, capaz de processar trilhÃµes de eventos por dia. Criado originalmente pelo LinkedIn e posteriormente doado Ã  Apache Software Foundation, Kafka se tornou o padrÃ£o de facto para streaming de dados em tempo real.

### CaracterÃ­sticas Principais

#### ğŸ“Š Alta Taxa de TransferÃªncia (High Throughput)
- Capaz de processar **milhÃµes de mensagens por segundo**
- LatÃªncia extremamente baixa (milissegundos)
- Escala horizontal com facilidade

#### ğŸ’¾ Durabilidade e PersistÃªncia
- Armazena mensagens em disco de forma durÃ¡vel
- ReplicaÃ§Ã£o de dados entre mÃºltiplos brokers
- TolerÃ¢ncia a falhas nativa

#### ğŸ”„ Processamento em Tempo Real
- Stream processing nativo
- IntegraÃ§Ã£o com Kafka Streams e ksqlDB
- Processamento de eventos complexos (CEP)

#### ğŸ“ˆ Escalabilidade Horizontal
- Adicione brokers conforme a demanda cresce
- Particionamento automÃ¡tico de tÃ³picos
- DistribuiÃ§Ã£o de carga entre consumidores

### Casos de Uso Comuns

#### 1. **Mensageria entre Microservices**
```
Order Service â†’ Kafka â†’ [Payment Service, Inventory Service, Notification Service]
```

#### 2. **Event Sourcing e CQRS**
- Armazenar todos os eventos como fonte da verdade
- Reconstruir estado a qualquer momento
- SeparaÃ§Ã£o de leitura e escrita

#### 3. **Log Aggregation**
- Centralizar logs de mÃºltiplas aplicaÃ§Ãµes
- Processamento e anÃ¡lise em tempo real
- IntegraÃ§Ã£o com Elasticsearch, Splunk, etc.

#### 4. **Stream Processing**
- AnÃ¡lise de dados em tempo real
- DetecÃ§Ã£o de fraudes
- Monitoramento de IoT

#### 5. **Data Pipeline**
- ETL em tempo real
- SincronizaÃ§Ã£o entre sistemas
- Data Lake ingestion

---

## 2. Conceitos Fundamentais do Kafka

### 2.1. Arquitetura do Kafka

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KAFKA CLUSTER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Broker 1   â”‚  â”‚   Broker 2   â”‚  â”‚   Broker 3   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚  Topic A     â”‚  â”‚  Topic A     â”‚  â”‚  Topic B     â”‚      â”‚
â”‚  â”‚  Partition 0 â”‚  â”‚  Partition 1 â”‚  â”‚  Partition 0 â”‚      â”‚
â”‚  â”‚  (Leader)    â”‚  â”‚  (Leader)    â”‚  â”‚  (Leader)    â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚  Topic B     â”‚  â”‚  Topic A     â”‚  â”‚  Topic A     â”‚      â”‚
â”‚  â”‚  Partition 1 â”‚  â”‚  Partition 0 â”‚  â”‚  Partition 1 â”‚      â”‚
â”‚  â”‚  (Replica)   â”‚  â”‚  (Replica)   â”‚  â”‚  (Replica)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â–²                                           â”‚
        â”‚ Produce                          Consume  â”‚
        â”‚                                           â–¼

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Producer â”‚                              â”‚ Consumer â”‚
   â”‚  App 1   â”‚                              â”‚ Group A  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2. Componentes Essenciais

#### **Producer (Produtor)**
> AplicaÃ§Ã£o que publica (envia) mensagens para tÃ³picos Kafka.

**Responsabilidades:**
- Serializar dados
- Escolher partiÃ§Ã£o (por chave ou round-robin)
- Gerenciar buffers e batches
- Lidar com retries e confirmaÃ§Ãµes

#### **Consumer (Consumidor)**
> AplicaÃ§Ã£o que lÃª mensagens de um ou mais tÃ³picos Kafka.

**Responsabilidades:**
- Deserializar dados
- Gerenciar offsets (posiÃ§Ã£o de leitura)
- Fazer parte de um consumer group
- Processar mensagens

#### **Broker**
> Servidor Kafka que armazena dados e atende requisiÃ§Ãµes.

**Responsabilidades:**
- Armazenar mensagens em disco
- Replicar dados entre brokers
- Gerenciar lÃ­deres e rÃ©plicas de partiÃ§Ãµes
- Servir requisiÃ§Ãµes de producers e consumers

#### **Topic (TÃ³pico)**
> Categoria ou nome de feed onde mensagens sÃ£o publicadas.

**CaracterÃ­sticas:**
- Similar a uma tabela em banco de dados
- Multi-subscriber (vÃ¡rios consumidores)
- Pode ter mÃºltiplas partiÃ§Ãµes
- ConfigurÃ¡vel (retenÃ§Ã£o, replicaÃ§Ã£o, etc.)

#### **Partition (PartiÃ§Ã£o)**
> SubdivisÃ£o de um tÃ³pico para paralelizaÃ§Ã£o e escalabilidade.

**CaracterÃ­sticas:**
- OrdenaÃ§Ã£o garantida **dentro da partiÃ§Ã£o**
- Cada partiÃ§Ã£o tem um lÃ­der e rÃ©plicas
- DistribuÃ­das entre brokers
- Permite processamento paralelo

**Exemplo de Particionamento:**
```
Topic: orders
â”œâ”€â”€ Partition 0: [msg1, msg4, msg7, msg10]  â†’ Consumer 1
â”œâ”€â”€ Partition 1: [msg2, msg5, msg8, msg11]  â†’ Consumer 2
â””â”€â”€ Partition 2: [msg3, msg6, msg9, msg12]  â†’ Consumer 3
```

#### **Offset**
> Identificador Ãºnico sequencial de cada mensagem dentro de uma partiÃ§Ã£o.

**CaracterÃ­sticas:**
- NÃºmero inteiro incremental (0, 1, 2, 3...)
- Ãšnico por partiÃ§Ã£o
- Permite replay de mensagens
- Gerenciado automaticamente ou manualmente

#### **Consumer Group**
> Grupo de consumidores que trabalham juntos para processar um tÃ³pico.

**CaracterÃ­sticas:**
- Cada partiÃ§Ã£o Ã© consumida por apenas **um** consumidor do grupo
- Permite escalabilidade horizontal
- Rebalanceamento automÃ¡tico quando consumidores entram/saem
- Garante que cada mensagem seja processada uma vez por grupo

**Exemplo:**
```
Topic: payments (3 partiÃ§Ãµes)

Consumer Group "payment-processor":
â”œâ”€â”€ Consumer A â†’ Partition 0
â”œâ”€â”€ Consumer B â†’ Partition 1
â””â”€â”€ Consumer C â†’ Partition 2

Consumer Group "audit-service":
â”œâ”€â”€ Consumer D â†’ Partitions 0, 1, 2
```

### 2.3. Garantias de Entrega

#### **At Most Once (No mÃ¡ximo uma vez)**
- Mensagem pode ser perdida
- Nunca serÃ¡ processada mais de uma vez
- Menor latÃªncia
```properties
enable.auto.commit=true
acks=0
```

#### **At Least Once (Pelo menos uma vez)**
- Mensagem nunca Ã© perdida
- Pode ser processada mais de uma vez
- Requer idempotÃªncia na aplicaÃ§Ã£o
```properties
enable.auto.commit=false
acks=all
retries=3
```

#### **Exactly Once (Exatamente uma vez)**
- Mensagem Ã© processada exatamente uma vez
- Maior complexidade
- Usa transaÃ§Ãµes do Kafka
```properties
enable.idempotence=true
transactional.id=unique-id
acks=all
```

### 2.4. RetenÃ§Ã£o de Dados

Kafka pode reter mensagens baseado em:

#### **Tempo (Time-based)**
```properties
# Reter por 7 dias
retention.ms=604800000
```

#### **Tamanho (Size-based)**
```properties
# Reter atÃ© 1GB por partiÃ§Ã£o
retention.bytes=1073741824
```

#### **CompactaÃ§Ã£o (Log Compaction)**
- MantÃ©m apenas a Ãºltima versÃ£o de cada chave
- Ãštil para change data capture (CDC)
```properties
cleanup.policy=compact
```

---

## 3. Kafka + Quarkus: A CombinaÃ§Ã£o Perfeita

### Por que usar Kafka com Quarkus?

#### âš¡ **Performance Excepcional**

**Quarkus:**
- InicializaÃ§Ã£o em milissegundos (0.042s)
- Consumo de memÃ³ria reduzido (70% menos que frameworks tradicionais)
- CompilaÃ§Ã£o nativa com GraalVM

**Kafka:**
- Throughput massivo (milhÃµes de msg/s)
- LatÃªncia baixa (< 10ms)
- Processamento em tempo real

**Resultado:**
```
AplicaÃ§Ã£o tradicional:  Startup: 10s  | Memory: 200MB
AplicaÃ§Ã£o Quarkus:      Startup: 0.05s | Memory: 30MB
```

#### ğŸ”„ **Reatividade Nativa**

Quarkus utiliza **SmallRye Reactive Messaging** que implementa a especificaÃ§Ã£o MicroProfile Reactive Messaging, oferecendo:

- **Non-blocking I/O**: Processamento assÃ­ncrono eficiente
- **Backpressure**: Controle de fluxo automÃ¡tico
- **Reactive Streams**: Interoperabilidade com outras bibliotecas reativas

```java
@Incoming("orders")
@Outgoing("processed-orders")
public Uni<Order> processOrder(Order order) {
    return orderService.validate(order)
        .onItem().transform(this::enrich)
        .onItem().invoke(this::audit);
}
```

#### ğŸ¯ **Developer Experience Superior**

- **Live Coding**: MudanÃ§as refletidas instantaneamente
- **Dev Services**: Kafka iniciado automaticamente em dev mode
- **ConfiguraÃ§Ã£o Declarativa**: Menos cÃ³digo, mais produtividade

```bash
# Inicia Kafka automaticamente
./mvnw quarkus:dev
```

#### ğŸ³ **Cloud-Native por Design**

- **Containers Otimizados**: Imagens pequenas (< 100MB)
- **Kubernetes-Ready**: IntegraÃ§Ã£o nativa com K8s
- **Health Checks**: Liveness e readiness automÃ¡ticos
- **Metrics**: Prometheus out-of-the-box

#### ğŸ“¦ **Ecossistema Rico**

```xml
<!-- Reactive Messaging -->
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
</dependency>

<!-- Kafka Streams -->
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-kafka-streams</artifactId>
</dependency>

<!-- Schema Registry (Avro) -->
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-confluent-registry-avro</artifactId>
</dependency>
```

### BenefÃ­cios da IntegraÃ§Ã£o Nativa

#### 1. **ConfiguraÃ§Ã£o Simplificada**

**Antes (Kafka Client puro):**
```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("acks", "all");
// ... 20+ linhas de configuraÃ§Ã£o

KafkaProducer<String, String> producer = new KafkaProducer<>(props);
producer.send(new ProducerRecord<>("topic", "key", "value"));
```

**Com Quarkus:**
```properties
mp.messaging.outgoing.orders.connector=smallrye-kafka
mp.messaging.outgoing.orders.topic=orders
```

```java
@Inject @Channel("orders") Emitter<Order> emitter;
emitter.send(order); // Pronto!
```

#### 2. **InjeÃ§Ã£o de DependÃªncias**

```java
@ApplicationScoped
public class OrderService {
    
    @Inject @Channel("orders") 
    Emitter<Order> orderEmitter;
    
    @Inject @Channel("notifications") 
    Emitter<Notification> notificationEmitter;
    
    @Inject
    OrderRepository repository;
    
    @Transactional
    public void createOrder(Order order) {
        repository.persist(order);
        orderEmitter.send(order);
        notificationEmitter.send(new Notification(order));
    }
}
```

#### 3. **Health Checks AutomÃ¡ticos**

```bash
$ curl http://localhost:8080/q/health

{
  "status": "UP",
  "checks": [
    {
      "name": "SmallRye Reactive Messaging - liveness check",
      "status": "UP"
    },
    {
      "name": "SmallRye Reactive Messaging - readiness check",
      "status": "UP",
      "data": {
        "orders-in": "[OK]",
        "orders-out": "[OK]"
      }
    }
  ]
}
```

#### 4. **Desenvolvimento Facilitado**

```bash
# Dev mode com Kafka automÃ¡tico
./mvnw quarkus:dev

# Quarkus inicia container Kafka automaticamente!
# Nenhuma configuraÃ§Ã£o manual necessÃ¡ria
```

#### 5. **Observabilidade Integrada**

```properties
# MÃ©tricas automÃ¡ticas
quarkus.micrometer.enabled=true
quarkus.micrometer.export.prometheus.enabled=true
```

MÃ©tricas disponÃ­veis automaticamente:
- `kafka_consumer_fetch_manager_records_consumed_total`
- `kafka_producer_topic_record_send_total`
- `kafka_consumer_coordinator_commit_latency_avg`
- E muitas outras...

### Arquitetura Recomendada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Quarkus Application                      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   REST API   â”‚â”€â”€â”€â”€â–¶â”‚   Service    â”‚â”€â”€â”€â”€â–¶â”‚  Producer  â”‚ â”‚
â”‚  â”‚  Controller  â”‚     â”‚    Layer     â”‚     â”‚  (Emitter) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                   â”‚        â”‚
â”‚                                                   â–¼        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Kafka Cluster  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Consumer    â”‚       â”‚   Consumer    â”‚
â”‚   Service 1   â”‚       â”‚   Service 2   â”‚
â”‚  (Quarkus)    â”‚       â”‚  (Quarkus)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Setup do Projeto Quarkus

### 4.1. Criando o Projeto

#### OpÃ§Ã£o 1: Quarkus CLI (Recomendado)

```bash
# Instalar Quarkus CLI (se ainda nÃ£o tiver)
# Windows (PowerShell)
iex "& { $(irm https://ps.jbang.dev) } app install --fresh --force quarkus@quarkusio"

# Criar projeto
quarkus create app com.example:kafka-demo \
    --extension='quarkus-smallrye-reactive-messaging-kafka,quarkus-rest-jackson,quarkus-arc' \
    --java=21
```

#### OpÃ§Ã£o 2: Maven

```bash
mvn io.quarkus.platform:quarkus-maven-plugin:3.15.1:create \
    -DprojectGroupId=com.example \
    -DprojectArtifactId=kafka-demo \
    -DprojectVersion=1.0.0-SNAPSHOT \
    -Dextensions="smallrye-reactive-messaging-kafka,rest-jackson,arc" \
    -DjavaVersion=21
```

#### OpÃ§Ã£o 3: Maven (Projeto Existente)

```bash
cd seu-projeto
./mvnw quarkus:add-extension -Dextensions="smallrye-reactive-messaging-kafka"
```

#### OpÃ§Ã£o 4: Code.quarkus.io

1. Acesse https://code.quarkus.io
2. Configure:
   - **Group:** com.example
   - **Artifact:** kafka-demo
   - **Build Tool:** Maven
   - **Java Version:** 21
3. Adicione extensÃµes:
   - SmallRye Reactive Messaging - Kafka
   - REST
   - REST Jackson
4. Clique em "Generate your application"

### 4.2. Estrutura do Projeto

```
kafka-demo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ java/
â”‚   â”‚   â”‚   â””â”€â”€ com/
â”‚   â”‚   â”‚       â””â”€â”€ example/
â”‚   â”‚   â”‚           â”œâ”€â”€ model/
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ Order.java
â”‚   â”‚   â”‚           â”œâ”€â”€ producer/
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ OrderProducer.java
â”‚   â”‚   â”‚           â”œâ”€â”€ consumer/
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ OrderConsumer.java
â”‚   â”‚   â”‚           â”œâ”€â”€ resource/
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ OrderResource.java
â”‚   â”‚   â”‚           â””â”€â”€ config/
â”‚   â”‚   â”‚               â””â”€â”€ KafkaConfig.java
â”‚   â”‚   â””â”€â”€ resources/
â”‚   â”‚       â”œâ”€â”€ application.properties
â”‚   â”‚       â””â”€â”€ application-prod.properties
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ java/
â”‚           â””â”€â”€ com/
â”‚               â””â”€â”€ example/
â”‚                   â””â”€â”€ OrderResourceTest.java
â”œâ”€â”€ pom.xml
â””â”€â”€ docker-compose.yml
```

### 4.3. DependÃªncias Essenciais (pom.xml)

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    
    <groupId>com.example</groupId>
    <artifactId>kafka-demo</artifactId>
    <version>1.0.0-SNAPSHOT</version>
    
    <properties>
        <quarkus.platform.version>3.15.1</quarkus.platform.version>
        <maven.compiler.release>21</maven.compiler.release>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>
    
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>io.quarkus.platform</groupId>
                <artifactId>quarkus-bom</artifactId>
                <version>${quarkus.platform.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>
    
    <dependencies>
        <!-- Core Kafka Reactive Messaging -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
        </dependency>
        
        <!-- REST API -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest</artifactId>
        </dependency>
        
        <!-- JSON Serialization -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest-jackson</artifactId>
        </dependency>
        
        <!-- CDI (Dependency Injection) -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-arc</artifactId>
        </dependency>
        
        <!-- Health Checks -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-smallrye-health</artifactId>
        </dependency>
        
        <!-- Metrics -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
        </dependency>
        
        <!-- Logging -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-logging-json</artifactId>
        </dependency>
        
        <!-- Bean Validation -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-hibernate-validator</artifactId>
        </dependency>
        
        <!-- Testing -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-junit5</artifactId>
            <scope>test</scope>
        </dependency>
        
        <dependency>
            <groupId>io.rest-assured</groupId>
            <artifactId>rest-assured</artifactId>
            <scope>test</scope>
        </dependency>
        
        <!-- In-Memory Kafka for Tests -->
        <dependency>
            <groupId>io.smallrye.reactive</groupId>
            <artifactId>smallrye-reactive-messaging-in-memory</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>
    
    <build>
        <plugins>
            <plugin>
                <groupId>io.quarkus.platform</groupId>
                <artifactId>quarkus-maven-plugin</artifactId>
                <version>${quarkus.platform.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>build</goal>
                            <goal>generate-code</goal>
                            <goal>generate-code-tests</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

### 4.4. ConfiguraÃ§Ã£o Completa (application.properties)

```properties
# ============================================================================
# APPLICATION
# ============================================================================
quarkus.application.name=kafka-demo
quarkus.application.version=1.0.0

# ============================================================================
# KAFKA BOOTSTRAP CONFIGURATION
# ============================================================================
kafka.bootstrap.servers=localhost:9092

# ============================================================================
# PRODUCER CONFIGURATION - Orders Out
# ============================================================================
# Connector Configuration
mp.messaging.outgoing.orders-out.connector=smallrye-kafka
mp.messaging.outgoing.orders-out.topic=orders
mp.messaging.outgoing.orders-out.key.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.orders-out.value.serializer=io.quarkus.kafka.client.serialization.ObjectMapperSerializer

# Producer Performance
mp.messaging.outgoing.orders-out.acks=all
mp.messaging.outgoing.orders-out.retries=3
mp.messaging.outgoing.orders-out.batch.size=16384
mp.messaging.outgoing.orders-out.linger.ms=5
mp.messaging.outgoing.orders-out.compression.type=snappy
mp.messaging.outgoing.orders-out.max.in.flight.requests.per.connection=5

# Idempotence
mp.messaging.outgoing.orders-out.enable.idempotence=true

# ============================================================================
# CONSUMER CONFIGURATION - Orders In
# ============================================================================
# Connector Configuration
mp.messaging.incoming.orders-in.connector=smallrye-kafka
mp.messaging.incoming.orders-in.topic=orders
mp.messaging.incoming.orders-in.group.id=order-processor-group
mp.messaging.incoming.orders-in.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders-in.value.deserializer=io.quarkus.kafka.client.serialization.ObjectMapperDeserializer

# Deserialization Type
mp.messaging.incoming.orders-in.value.deserializer.type=com.example.model.Order

# Offset Management
mp.messaging.incoming.orders-in.auto.offset.reset=earliest
mp.messaging.incoming.orders-in.enable.auto.commit=false
mp.messaging.incoming.orders-in.commit-strategy=throttled

# Consumer Performance
mp.messaging.incoming.orders-in.fetch.min.bytes=1024
mp.messaging.incoming.orders-in.fetch.max.wait.ms=500
mp.messaging.incoming.orders-in.max.poll.records=500
mp.messaging.incoming.orders-in.session.timeout.ms=30000
mp.messaging.incoming.orders-in.heartbeat.interval.ms=3000

# Failure Handling
mp.messaging.incoming.orders-in.failure-strategy=ignore
mp.messaging.incoming.orders-in.throttled.unprocessed-record-max-age.ms=10000

# ============================================================================
# DEAD LETTER QUEUE (DLQ)
# ============================================================================
mp.messaging.outgoing.orders-dlq.connector=smallrye-kafka
mp.messaging.outgoing.orders-dlq.topic=orders-dlq
mp.messaging.outgoing.orders-dlq.value.serializer=org.apache.kafka.common.serialization.StringSerializer

# ============================================================================
# LOGGING
# ============================================================================
quarkus.log.level=INFO
quarkus.log.category."org.apache.kafka".level=WARN
quarkus.log.category."com.example".level=DEBUG
quarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n

# JSON Logging (Production)
quarkus.log.console.json=false

# ============================================================================
# HEALTH CHECKS
# ============================================================================
quarkus.smallrye-health.root-path=/health
quarkus.smallrye-health.liveness-path=/live
quarkus.smallrye-health.readiness-path=/ready

# ============================================================================
# METRICS
# ============================================================================
quarkus.micrometer.enabled=true
quarkus.micrometer.export.prometheus.enabled=true
quarkus.micrometer.export.prometheus.path=/metrics
quarkus.micrometer.binder.kafka.enabled=true

# ============================================================================
# DEVELOPMENT MODE
# ============================================================================
%dev.quarkus.log.level=DEBUG
%dev.quarkus.log.category."org.apache.kafka".level=DEBUG

# Dev Services - Kafka automÃ¡tico em modo dev
%dev.quarkus.kafka.devservices.enabled=true
%dev.quarkus.kafka.devservices.port=9092
%dev.quarkus.kafka.devservices.topic-partitions.orders=3
%dev.quarkus.kafka.devservices.topic-partitions.notifications=2

# ============================================================================
# TEST MODE
# ============================================================================
%test.quarkus.kafka.devservices.enabled=false

# ============================================================================
# PRODUCTION MODE
# ============================================================================
%prod.kafka.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS:kafka:9092}
%prod.quarkus.log.console.json=true
%prod.quarkus.log.level=WARN
%prod.quarkus.log.category."com.example".level=INFO
```

### 4.5. ConfiguraÃ§Ãµes Adicionais por Ambiente

#### application-dev.properties
```properties
# Desenvolvimento Local
kafka.bootstrap.servers=localhost:9092
quarkus.log.level=DEBUG
mp.messaging.incoming.orders-in.auto.offset.reset=earliest
```

#### application-prod.properties
```properties
# ProduÃ§Ã£o
kafka.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS}
quarkus.log.console.json=true
quarkus.log.level=WARN

# Security
kafka.security.protocol=SASL_SSL
kafka.sasl.mechanism=PLAIN
kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="${KAFKA_USERNAME}" password="${KAFKA_PASSWORD}";

# SSL
kafka.ssl.truststore.location=/etc/ssl/certs/kafka-truststore.jks
kafka.ssl.truststore.password=${TRUSTSTORE_PASSWORD}

# Performance
mp.messaging.incoming.orders-in.concurrency=5
mp.messaging.outgoing.orders-out.max-inflight-messages=1000
```

### 4.6. ValidaÃ§Ã£o da ConfiguraÃ§Ã£o

```bash
# Iniciar em modo dev
./mvnw quarkus:dev

# Verificar health
curl http://localhost:8080/q/health

# Verificar mÃ©tricas
curl http://localhost:8080/q/metrics

# Ver informaÃ§Ãµes da aplicaÃ§Ã£o
curl http://localhost:8080/q/info
```

---

## 5. ConfiguraÃ§Ã£o do Kafka

### 5.1. Docker Compose Completo (docker-compose.yml)

```yaml
version: '3.8'

# ============================================================================
# Kafka com KRaft (sem Zookeeper) + UI + Schema Registry
# ============================================================================

services:
  # --------------------------------------------------------------------------
  # Kafka Broker com KRaft
  # --------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    hostname: kafka
    container_name: kafka-broker
    ports:
      - "9092:9092"      # Cliente externo
      - "9101:9101"      # JMX Metrics
    environment:
      # KRaft Configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      
      # Listeners
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      
      # Cluster ID (gerado com: kafka-storage random-uuid)
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      
      # Storage
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      
      # Topic Defaults
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      
      # Performance
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      
      # Compression
      KAFKA_COMPRESSION_TYPE: 'snappy'
      
      # Retention
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      
      # JMX
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      
    volumes:
      - kafka-data:/tmp/kraft-combined-logs
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - kafka-network

  # --------------------------------------------------------------------------
  # Kafka UI (Interface GrÃ¡fica)
  # --------------------------------------------------------------------------
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local-kafka
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      - KAFKA_CLUSTERS_0_METRICS_PORT=9101
      - KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://schema-registry:8081
      - DYNAMIC_CONFIG_ENABLED=true
      - LOGGING_LEVEL_ROOT=INFO
      - LOGGING_LEVEL_COM_PROVECTUS=DEBUG
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - kafka-network

  # --------------------------------------------------------------------------
  # Schema Registry (para Avro/Protobuf)
  # --------------------------------------------------------------------------
  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.0
    hostname: schema-registry
    container_name: schema-registry
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_SCHEMA_REGISTRY_GROUP_ID: schema-registry
      SCHEMA_REGISTRY_STORAGE_TOPIC: _schemas
      SCHEMA_REGISTRY_STORAGE_REPLICATION_FACTOR: 1
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - kafka-network

  # --------------------------------------------------------------------------
  # Kafka Connect (opcional - para integraÃ§Ã£o com outros sistemas)
  # --------------------------------------------------------------------------
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.6.0
    hostname: kafka-connect
    container_name: kafka-connect
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - kafka-network

volumes:
  kafka-data:
    driver: local

networks:
  kafka-network:
    driver: bridge
```

### 5.2. Iniciando o Ambiente

```powershell
# Iniciar todos os serviÃ§os
docker-compose up -d

# Verificar status
docker-compose ps

# Ver logs do Kafka
docker-compose logs -f kafka

# Ver logs de todos os serviÃ§os
docker-compose logs -f

# Parar todos os serviÃ§os
docker-compose down

# Parar e remover volumes (limpar dados)
docker-compose down -v
```

### 5.3. Gerenciamento de TÃ³picos

#### Criar TÃ³picos via CLI

```powershell
# TÃ³pico para pedidos (3 partiÃ§Ãµes)
docker exec -it kafka-broker kafka-topics --create `
    --topic orders `
    --bootstrap-server localhost:9092 `
    --partitions 3 `
    --replication-factor 1 `
    --config retention.ms=604800000 `
    --config compression.type=snappy

# TÃ³pico para notificaÃ§Ãµes (2 partiÃ§Ãµes)
docker exec -it kafka-broker kafka-topics --create `
    --topic notifications `
    --bootstrap-server localhost:9092 `
    --partitions 2 `
    --replication-factor 1

# TÃ³pico para auditoria (log compaction)
docker exec -it kafka-broker kafka-topics --create `
    --topic audit-log `
    --bootstrap-server localhost:9092 `
    --partitions 1 `
    --replication-factor 1 `
    --config cleanup.policy=compact

# Dead Letter Queue
docker exec -it kafka-broker kafka-topics --create `
    --topic orders-dlq `
    --bootstrap-server localhost:9092 `
    --partitions 1 `
    --replication-factor 1
```

#### Listar TÃ³picos

```powershell
docker exec -it kafka-broker kafka-topics --list `
    --bootstrap-server localhost:9092
```

#### Descrever TÃ³pico

```powershell
docker exec -it kafka-broker kafka-topics --describe `
    --topic orders `
    --bootstrap-server localhost:9092
```

**SaÃ­da esperada:**
```
Topic: orders	TopicId: xyz123	PartitionCount: 3	ReplicationFactor: 1
	Topic: orders	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
	Topic: orders	Partition: 1	Leader: 1	Replicas: 1	Isr: 1
	Topic: orders	Partition: 2	Leader: 1	Replicas: 1	Isr: 1
```

#### Alterar ConfiguraÃ§Ãµes

```powershell
# Aumentar retenÃ§Ã£o para 30 dias
docker exec -it kafka-broker kafka-configs --alter `
    --entity-type topics `
    --entity-name orders `
    --add-config retention.ms=2592000000 `
    --bootstrap-server localhost:9092
```

#### Adicionar PartiÃ§Ãµes

```powershell
docker exec -it kafka-broker kafka-topics --alter `
    --topic orders `
    --partitions 5 `
    --bootstrap-server localhost:9092
```

#### Deletar TÃ³pico

```powershell
docker exec -it kafka-broker kafka-topics --delete `
    --topic orders `
    --bootstrap-server localhost:9092
```

### 5.4. Produzir e Consumir Mensagens via CLI (Para Testes)

#### Producer Console

```powershell
# Produzir mensagens
docker exec -it kafka-broker kafka-console-producer `
    --topic orders `
    --bootstrap-server localhost:9092

# Digite as mensagens (Enter para enviar)
> {"id": "1", "product": "Laptop", "amount": 1500}
> {"id": "2", "product": "Mouse", "amount": 25}
```

#### Producer com Chave

```powershell
docker exec -it kafka-broker kafka-console-producer `
    --topic orders `
    --bootstrap-server localhost:9092 `
    --property "parse.key=true" `
    --property "key.separator=:"

# Formato: chave:valor
> customer-123:{"id": "1", "product": "Laptop"}
> customer-456:{"id": "2", "product": "Mouse"}
```

#### Consumer Console

```powershell
# Consumir do inÃ­cio
docker exec -it kafka-broker kafka-console-consumer `
    --topic orders `
    --bootstrap-server localhost:9092 `
    --from-beginning

# Consumir com chave
docker exec -it kafka-broker kafka-console-consumer `
    --topic orders `
    --bootstrap-server localhost:9092 `
    --from-beginning `
    --property print.key=true `
    --property print.timestamp=true `
    --property key.separator=" | "
```

#### Consumer Group

```powershell
docker exec -it kafka-broker kafka-console-consumer `
    --topic orders `
    --bootstrap-server localhost:9092 `
    --group my-consumer-group `
    --from-beginning
```

### 5.5. Gerenciamento de Consumer Groups

#### Listar Consumer Groups

```powershell
docker exec -it kafka-broker kafka-consumer-groups --list `
    --bootstrap-server localhost:9092
```

#### Descrever Consumer Group

```powershell
docker exec -it kafka-broker kafka-consumer-groups --describe `
    --group order-processor-group `
    --bootstrap-server localhost:9092
```

**SaÃ­da esperada:**
```
GROUP                TOPIC       PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
order-processor-group orders     0          100             100             0
order-processor-group orders     1          150             155             5
order-processor-group orders     2          200             200             0
```

#### Reset Offset (Cuidado em ProduÃ§Ã£o!)

```powershell
# Reset para o inÃ­cio
docker exec -it kafka-broker kafka-consumer-groups --reset-offsets `
    --group order-processor-group `
    --topic orders `
    --to-earliest `
    --execute `
    --bootstrap-server localhost:9092

# Reset para offset especÃ­fico
docker exec -it kafka-broker kafka-consumer-groups --reset-offsets `
    --group order-processor-group `
    --topic orders:0 `
    --to-offset 50 `
    --execute `
    --bootstrap-server localhost:9092

# Reset para timestamp
docker exec -it kafka-broker kafka-consumer-groups --reset-offsets `
    --group order-processor-group `
    --topic orders `
    --to-datetime 2025-09-30T10:00:00.000 `
    --execute `
    --bootstrap-server localhost:9092
```

### 5.6. Monitoramento via Kafka UI

Acesse: **http://localhost:8080**

Funcionalidades disponÃ­veis:
- ğŸ“Š **Brokers**: Status e configuraÃ§Ã£o
- ğŸ“ **Topics**: Gerenciamento visual de tÃ³picos
- ğŸ‘¥ **Consumer Groups**: Monitoramento de lag e offset
- ğŸ“¨ **Messages**: Visualizar e produzir mensagens
- ğŸ“ˆ **Metrics**: GrÃ¡ficos de performance
- âš™ï¸ **Configuration**: Alterar configuraÃ§Ãµes dinamicamente

### 5.7. Script de InicializaÃ§Ã£o (init-kafka.sh)

Crie um script para automatizar a criaÃ§Ã£o de tÃ³picos:

```bash
#!/bin/bash

echo "Aguardando Kafka inicializar..."
sleep 10

echo "Criando tÃ³picos..."

docker exec -it kafka-broker kafka-topics --create \
    --if-not-exists \
    --topic orders \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1

docker exec -it kafka-broker kafka-topics --create \
    --if-not-exists \
    --topic notifications \
    --bootstrap-server localhost:9092 \
    --partitions 2 \
    --replication-factor 1

docker exec -it kafka-broker kafka-topics --create \
    --if-not-exists \
    --topic orders-dlq \
    --bootstrap-server localhost:9092 \
    --partitions 1 \
    --replication-factor 1

echo "TÃ³picos criados com sucesso!"
docker exec -it kafka-broker kafka-topics --list --bootstrap-server localhost:9092
```

```powershell
# PowerShell equivalente (init-kafka.ps1)
Write-Host "Aguardando Kafka inicializar..."
Start-Sleep -Seconds 10

Write-Host "Criando tÃ³picos..."

docker exec kafka-broker kafka-topics --create `
    --if-not-exists `
    --topic orders `
    --bootstrap-server localhost:9092 `
    --partitions 3 `
    --replication-factor 1

docker exec kafka-broker kafka-topics --create `
    --if-not-exists `
    --topic notifications `
    --bootstrap-server localhost:9092 `
    --partitions 2 `
    --replication-factor 1

docker exec kafka-broker kafka-topics --create `
    --if-not-exists `
    --topic orders-dlq `
    --bootstrap-server localhost:9092 `
    --partitions 1 `
    --replication-factor 1

Write-Host "TÃ³picos criados com sucesso!"
docker exec kafka-broker kafka-topics --list --bootstrap-server localhost:9092
```

---

## 6. Produzindo Mensagens

### 6.1. Modelo de Dados (com ValidaÃ§Ã£o)

```java
package com.example.model;

import com.fasterxml.jackson.annotation.JsonFormat;
import jakarta.validation.constraints.*;
import java.math.BigDecimal;
import java.time.LocalDateTime;
import java.util.Objects;
import java.util.UUID;

/**
 * Representa um pedido no sistema de e-commerce.
 * 
 * <p>Esta classe Ã© utilizada para comunicaÃ§Ã£o via Kafka entre microservices.
 * Inclui validaÃ§Ãµes Bean Validation para garantir integridade dos dados.</p>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
public class Order {
    
    /**
     * Identificador Ãºnico do pedido (UUID).
     */
    @NotBlank(message = "ID do pedido nÃ£o pode ser vazio")
    private String id;
    
    /**
     * Identificador do cliente que realizou o pedido.
     */
    @NotBlank(message = "ID do cliente Ã© obrigatÃ³rio")
    @Size(min = 3, max = 50, message = "ID do cliente deve ter entre 3 e 50 caracteres")
    private String customerId;
    
    /**
     * Nome do produto.
     */
    @NotBlank(message = "Nome do produto Ã© obrigatÃ³rio")
    @Size(min = 2, max = 200, message = "Nome do produto deve ter entre 2 e 200 caracteres")
    private String product;
    
    /**
     * Valor total do pedido.
     */
    @NotNull(message = "Valor do pedido Ã© obrigatÃ³rio")
    @DecimalMin(value = "0.01", message = "Valor deve ser maior que zero")
    @Digits(integer = 10, fraction = 2, message = "Valor invÃ¡lido")
    private BigDecimal amount;
    
    /**
     * Quantidade de itens.
     */
    @Min(value = 1, message = "Quantidade mÃ­nima Ã© 1")
    @Max(value = 9999, message = "Quantidade mÃ¡xima Ã© 9999")
    private Integer quantity;
    
    /**
     * Status do pedido.
     */
    @NotNull(message = "Status Ã© obrigatÃ³rio")
    private OrderStatus status;
    
    /**
     * Timestamp de criaÃ§Ã£o do pedido.
     */
    @JsonFormat(pattern = "yyyy-MM-dd'T'HH:mm:ss")
    private LocalDateTime timestamp;
    
    /**
     * Email do cliente (para notificaÃ§Ãµes).
     */
    @Email(message = "Email invÃ¡lido")
    private String customerEmail;
    
    /**
     * Construtor padrÃ£o (necessÃ¡rio para deserializaÃ§Ã£o).
     */
    public Order() {
        this.timestamp = LocalDateTime.now();
        this.status = OrderStatus.PENDING;
    }
    
    /**
     * Construtor completo para criaÃ§Ã£o de pedidos.
     *
     * @param customerId ID do cliente
     * @param product Nome do produto
     * @param amount Valor total
     * @param quantity Quantidade
     * @param customerEmail Email do cliente
     */
    public Order(String customerId, String product, BigDecimal amount, 
                 Integer quantity, String customerEmail) {
        this();
        this.id = UUID.randomUUID().toString();
        this.customerId = customerId;
        this.product = product;
        this.amount = amount;
        this.quantity = quantity;
        this.customerEmail = customerEmail;
    }
    
    // Getters e Setters
    
    public String getId() {
        return id;
    }
    
    public void setId(String id) {
        this.id = id;
    }
    
    public String getCustomerId() {
        return customerId;
    }
    
    public void setCustomerId(String customerId) {
        this.customerId = customerId;
    }
    
    public String getProduct() {
        return product;
    }
    
    public void setProduct(String product) {
        this.product = product;
    }
    
    public BigDecimal getAmount() {
        return amount;
    }
    
    public void setAmount(BigDecimal amount) {
        this.amount = amount;
    }
    
    public Integer getQuantity() {
        return quantity;
    }
    
    public void setQuantity(Integer quantity) {
        this.quantity = quantity;
    }
    
    public OrderStatus getStatus() {
        return status;
    }
    
    public void setStatus(OrderStatus status) {
        this.status = status;
    }
    
    public LocalDateTime getTimestamp() {
        return timestamp;
    }
    
    public void setTimestamp(LocalDateTime timestamp) {
        this.timestamp = timestamp;
    }
    
    public String getCustomerEmail() {
        return customerEmail;
    }
    
    public void setCustomerEmail(String customerEmail) {
        this.customerEmail = customerEmail;
    }
    
    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        Order order = (Order) o;
        return Objects.equals(id, order.id);
    }
    
    @Override
    public int hashCode() {
        return Objects.hash(id);
    }
    
    @Override
    public String toString() {
        return "Order{" +
                "id='" + id + '\'' +
                ", customerId='" + customerId + '\'' +
                ", product='" + product + '\'' +
                ", amount=" + amount +
                ", quantity=" + quantity +
                ", status=" + status +
                ", timestamp=" + timestamp +
                '}';
    }
}
```

```java
package com.example.model;

/**
 * Status possÃ­veis de um pedido.
 */
public enum OrderStatus {
    /**
     * Pedido criado, aguardando processamento.
     */
    PENDING,
    
    /**
     * Pedido confirmado e em processamento.
     */
    CONFIRMED,
    
    /**
     * Pedido pago com sucesso.
     */
    PAID,
    
    /**
     * Pedido enviado para entrega.
     */
    SHIPPED,
    
    /**
     * Pedido entregue ao cliente.
     */
    DELIVERED,
    
    /**
     * Pedido cancelado.
     */
    CANCELLED,
    
    /**
     * Pedido com erro no processamento.
     */
    FAILED
}
```

### 6.2. Producer Service (Completo e Documentado)

```java
package com.example.producer;

import com.example.model.Order;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.smallrye.reactive.messaging.kafka.api.OutgoingKafkaRecordMetadata;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.Channel;
import org.eclipse.microprofile.reactive.messaging.Emitter;
import org.eclipse.microprofile.reactive.messaging.Message;
import org.eclipse.microprofile.reactive.messaging.Metadata;
import org.jboss.logging.Logger;

import java.util.concurrent.CompletionStage;

/**
 * ServiÃ§o responsÃ¡vel por produzir mensagens Kafka para pedidos.
 * 
 * <p>Utiliza o SmallRye Reactive Messaging para enviar mensagens de forma
 * assÃ­ncrona e nÃ£o bloqueante. Suporta envio com headers personalizados,
 * particionamento por chave e tratamento de erros.</p>
 * 
 * <h3>ConfiguraÃ§Ãµes Relacionadas:</h3>
 * <ul>
 *   <li>mp.messaging.outgoing.orders-out.connector=smallrye-kafka</li>
 *   <li>mp.messaging.outgoing.orders-out.topic=orders</li>
 * </ul>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class OrderProducer {
    
    private static final Logger LOG = Logger.getLogger(OrderProducer.class);
    
    /**
     * Emitter para envio de mensagens para o canal 'orders-out'.
     * Configurado via application.properties para publicar no tÃ³pico 'orders'.
     */
    @Inject
    @Channel("orders-out")
    Emitter<String> orderEmitter;
    
    /**
     * ObjectMapper para serializaÃ§Ã£o JSON.
     */
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Envia um pedido para o Kafka de forma simples.
     * 
     * <p>Este mÃ©todo serializa o pedido para JSON e envia para o tÃ³pico
     * configurado. O particionamento Ã© feito de forma round-robin.</p>
     * 
     * @param order Pedido a ser enviado
     * @throws RuntimeException se houver erro na serializaÃ§Ã£o ou envio
     */
    public void sendOrder(Order order) {
        try {
            String orderJson = objectMapper.writeValueAsString(order);
            
            orderEmitter.send(orderJson)
                .whenComplete((success, failure) -> {
                    if (failure != null) {
                        LOG.errorf("Erro ao enviar pedido %s: %s", 
                                  order.getId(), failure.getMessage());
                    } else {
                        LOG.infof("Pedido %s enviado com sucesso", order.getId());
                    }
                });
                
        } catch (JsonProcessingException e) {
            LOG.error("Erro na serializaÃ§Ã£o do pedido", e);
            throw new RuntimeException("Falha ao serializar pedido", e);
        }
    }
    
    /**
     * Envia um pedido com chave para garantir ordenaÃ§Ã£o por cliente.
     * 
     * <p>Mensagens com a mesma chave sÃ£o enviadas para a mesma partiÃ§Ã£o,
     * garantindo ordenaÃ§Ã£o. Ãštil para processar pedidos de um cliente
     * de forma sequencial.</p>
     * 
     * @param order Pedido a ser enviado
     * @return CompletionStage que completa quando a mensagem Ã© acknowledged
     */
    public CompletionStage<Void> sendOrderWithKey(Order order) {
        try {
            String orderJson = objectMapper.writeValueAsString(order);
            
            // Criar metadata com a chave (customerId)
            OutgoingKafkaRecordMetadata<String> metadata = 
                OutgoingKafkaRecordMetadata.<String>builder()
                    .withKey(order.getCustomerId())
                    .build();
            
            Message<String> message = Message.of(orderJson, Metadata.of(metadata));
            
            LOG.debugf("Enviando pedido %s com chave %s", 
                      order.getId(), order.getCustomerId());
            
            return orderEmitter.send(message);
            
        } catch (JsonProcessingException e) {
            LOG.error("Erro na serializaÃ§Ã£o do pedido", e);
            throw new RuntimeException("Falha ao serializar pedido", e);
        }
    }
    
    /**
     * Envia um pedido com headers personalizados e chave.
     * 
     * <p>Headers sÃ£o Ãºteis para filtros, roteamento e metadados adicionais
     * que nÃ£o fazem parte do payload principal.</p>
     * 
     * @param order Pedido a ser enviado
     * @return CompletionStage que completa quando a mensagem Ã© acknowledged
     */
    public CompletionStage<Void> sendOrderWithHeaders(Order order) {
        try {
            String orderJson = objectMapper.writeValueAsString(order);
            
            // Criar metadata com chave e headers
            OutgoingKafkaRecordMetadata<String> metadata = 
                OutgoingKafkaRecordMetadata.<String>builder()
                    .withKey(order.getCustomerId())
                    .withHeader("order-id", order.getId())
                    .withHeader("order-status", order.getStatus().name())
                    .withHeader("product", order.getProduct())
                    .withHeader("timestamp", order.getTimestamp().toString())
                    .withHeader("source", "order-service")
                    .withHeader("version", "1.0")
                    .build();
            
            Message<String> message = Message.of(orderJson, Metadata.of(metadata));
            
            LOG.infof("Enviando pedido %s com headers customizados", order.getId());
            
            return orderEmitter.send(message)
                .thenRun(() -> LOG.infof("Pedido %s confirmado pelo Kafka", order.getId()))
                .exceptionally(throwable -> {
                    LOG.errorf("Falha ao enviar pedido %s: %s", 
                              order.getId(), throwable.getMessage());
                    return null;
                });
            
        } catch (JsonProcessingException e) {
            LOG.error("Erro na serializaÃ§Ã£o do pedido", e);
            throw new RuntimeException("Falha ao serializar pedido", e);
        }
    }
    
    /**
     * Envia um pedido e aguarda a confirmaÃ§Ã£o de forma sÃ­ncrona.
     * 
     * <p><b>ATENÃ‡ÃƒO:</b> Este mÃ©todo bloqueia a thread atual atÃ© que o Kafka
     * confirme o recebimento. Use apenas quando necessÃ¡rio, pois pode impactar
     * a performance.</p>
     * 
     * @param order Pedido a ser enviado
     * @throws RuntimeException se houver erro no envio
     */
    public void sendOrderSync(Order order) {
        try {
            String orderJson = objectMapper.writeValueAsString(order);
            
            // toCompletableFuture().join() bloqueia atÃ© completar
            orderEmitter.send(orderJson).toCompletableFuture().join();
            
            LOG.infof("Pedido %s enviado e confirmado (sync)", order.getId());
            
        } catch (Exception e) {
            LOG.errorf("Erro ao enviar pedido %s (sync): %s", 
                      order.getId(), e.getMessage());
            throw new RuntimeException("Falha no envio sÃ­ncrono", e);
        }
    }
    
    /**
     * Envia mÃºltiplos pedidos de forma eficiente (batch).
     * 
     * @param orders Lista de pedidos
     */
    public void sendOrderBatch(java.util.List<Order> orders) {
        LOG.infof("Enviando batch de %d pedidos", orders.size());
        
        orders.forEach(order -> {
            try {
                sendOrderWithKey(order);
            } catch (Exception e) {
                LOG.errorf("Erro ao enviar pedido %s no batch: %s", 
                          order.getId(), e.getMessage());
            }
        });
        
        LOG.info("Batch enviado com sucesso");
    }
}
```

### 6.3. REST Controller para Producer

```java
package com.example.resource;

import com.example.model.Order;
import com.example.producer.OrderProducer;
import jakarta.inject.Inject;
import jakarta.validation.Valid;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import jakarta.ws.rs.core.Response;
import org.jboss.logging.Logger;

import java.net.URI;
import java.util.List;
import java.util.UUID;

/**
 * REST API para gerenciamento de pedidos.
 * 
 * <p>ExpÃµe endpoints para criaÃ§Ã£o de pedidos que sÃ£o publicados no Kafka
 * para processamento assÃ­ncrono por outros microservices.</p>
 * 
 * <h3>Endpoints:</h3>
 * <ul>
 *   <li>POST /api/orders - Criar pedido Ãºnico</li>
 *   <li>POST /api/orders/batch - Criar mÃºltiplos pedidos</li>
 *   <li>POST /api/orders/with-key - Criar pedido com chave</li>
 * </ul>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@Path("/api/orders")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public class OrderResource {
    
    private static final Logger LOG = Logger.getLogger(OrderResource.class);
    
    @Inject
    OrderProducer orderProducer;
    
    /**
     * Cria um novo pedido e envia para processamento.
     * 
     * <p>O pedido Ã© validado antes do envio. Se a validaÃ§Ã£o falhar,
     * retorna HTTP 400 com detalhes dos erros.</p>
     * 
     * @param order Dados do pedido
     * @return Response 202 (Accepted) com o pedido criado
     */
    @POST
    public Response createOrder(@Valid Order order) {
        LOG.infof("Recebida requisiÃ§Ã£o para criar pedido: %s", order.getProduct());
        
        // Gerar ID se nÃ£o foi fornecido
        if (order.getId() == null || order.getId().isBlank()) {
            order.setId(UUID.randomUUID().toString());
        }
        
        // Enviar para Kafka
        orderProducer.sendOrder(order);
        
        LOG.infof("Pedido %s aceito para processamento", order.getId());
        
        return Response.accepted(order)
                .location(URI.create("/api/orders/" + order.getId()))
                .build();
    }
    
    /**
     * Cria um pedido garantindo ordenaÃ§Ã£o por cliente.
     * 
     * @param order Dados do pedido
     * @return Response 202 (Accepted)
     */
    @POST
    @Path("/with-key")
    public Response createOrderWithKey(@Valid Order order) {
        LOG.infof("Criando pedido com chave para cliente: %s", order.getCustomerId());
        
        if (order.getId() == null || order.getId().isBlank()) {
            order.setId(UUID.randomUUID().toString());
        }
        
        orderProducer.sendOrderWithKey(order);
        
        return Response.accepted(order).build();
    }
    
    /**
     * Cria mÃºltiplos pedidos em lote.
     * 
     * @param orders Lista de pedidos
     * @return Response 202 (Accepted) com quantidade processada
     */
    @POST
    @Path("/batch")
    public Response createOrderBatch(@Valid List<Order> orders) {
        LOG.infof("Recebido batch de %d pedidos", orders.size());
        
        // Garantir que todos tÃªm ID
        orders.forEach(order -> {
            if (order.getId() == null || order.getId().isBlank()) {
                order.setId(UUID.randomUUID().toString());
            }
        });
        
        orderProducer.sendOrderBatch(orders);
        
        return Response.accepted()
                .entity(new BatchResponse(orders.size(), "Pedidos aceitos para processamento"))
                .build();
    }
    
    /**
     * DTO para resposta de operaÃ§Ãµes em lote.
     */
    public record BatchResponse(int count, String message) {}
}
```

### 6.4. Exemplos de Uso (cURL)

```powershell
# Criar pedido Ãºnico
curl -X POST http://localhost:8080/api/orders `
  -H "Content-Type: application/json" `
  -d '{
    "customerId": "customer-123",
    "product": "Laptop Dell XPS 15",
    "amount": 8500.00,
    "quantity": 1,
    "customerEmail": "cliente@example.com"
  }'

# Criar pedido com chave
curl -X POST http://localhost:8080/api/orders/with-key `
  -H "Content-Type: application/json" `
  -d '{
    "customerId": "customer-456",
    "product": "Mouse Logitech MX Master",
    "amount": 450.00,
    "quantity": 2,
    "customerEmail": "cliente2@example.com"
  }'

# Criar batch de pedidos
curl -X POST http://localhost:8080/api/orders/batch `
  -H "Content-Type: application/json" `
  -d '[
    {
      "customerId": "customer-789",
      "product": "Teclado MecÃ¢nico",
      "amount": 650.00,
      "quantity": 1,
      "customerEmail": "cliente3@example.com"
    },
    {
      "customerId": "customer-789",
      "product": "Monitor 4K",
      "amount": 2500.00,
      "quantity": 1,
      "customerEmail": "cliente3@example.com"
    }
  ]'
```

---

## 7. Consumindo Mensagens

### 7.1. Consumer BÃ¡sico (Fire and Forget)

```java
package com.example.consumer;

import com.example.model.Order;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.jboss.logging.Logger;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;

/**
 * Consumer bÃ¡sico de pedidos.
 * 
 * <p>Processa mensagens de forma assÃ­ncrona e retorna CompletionStage
 * para controle de fluxo reativo.</p>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class OrderConsumerBasic {
    
    private static final Logger LOG = Logger.getLogger(OrderConsumerBasic.class);
    
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Consome mensagens do tÃ³pico 'orders'.
     * 
     * <p>O mÃ©todo Ã© chamado automaticamente para cada mensagem recebida.
     * O offset Ã© commitado automaticamente apÃ³s o retorno bem-sucedido.</p>
     * 
     * @param orderJson JSON do pedido
     * @return CompletionStage que completa apÃ³s processamento
     */
    @Incoming("orders-in")
    public CompletionStage<Void> consume(String orderJson) {
        return CompletableFuture.runAsync(() -> {
            try {
                Order order = objectMapper.readValue(orderJson, Order.class);
                processOrder(order);
                LOG.infof("âœ“ Pedido %s processado com sucesso", order.getId());
            } catch (Exception e) {
                LOG.errorf("âœ— Erro ao processar pedido: %s", e.getMessage());
                // ExceÃ§Ãµes nÃ£o tratadas causarÃ£o nack e possivelmente retry
                throw new RuntimeException("Falha no processamento", e);
            }
        });
    }
    
    /**
     * LÃ³gica de negÃ³cio para processar o pedido.
     * 
     * @param order Pedido deserializado
     */
    private void processOrder(Order order) {
        LOG.infof("Processando pedido do cliente: %s", order.getCustomerId());
        LOG.debugf("Produto: %s | Qtd: %d | Valor: %s", 
                  order.getProduct(), order.getQuantity(), order.getAmount());
        
        // Simular processamento
        try {
            Thread.sleep(500);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        // Aqui entraria a lÃ³gica real: salvar em banco, chamar APIs, etc.
    }
}
```

### 7.2. Consumer com Metadata (AvanÃ§ado)

```java
package com.example.consumer;

import com.example.model.Order;
import com.example.model.OrderStatus;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.smallrye.reactive.messaging.kafka.api.IncomingKafkaRecordMetadata;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.apache.kafka.common.header.Header;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.eclipse.microprofile.reactive.messaging.Message;
import org.jboss.logging.Logger;

import java.nio.charset.StandardCharsets;
import java.time.Instant;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;

/**
 * Consumer avanÃ§ado com acesso a metadata do Kafka.
 * 
 * <p>Extrai informaÃ§Ãµes como partition, offset, headers e timestamp
 * para processamento contextual e auditoria.</p>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class OrderConsumerAdvanced {
    
    private static final Logger LOG = Logger.getLogger(OrderConsumerAdvanced.class);
    
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Consome mensagens com acesso completo a metadata.
     * 
     * @param message Mensagem completa com payload e metadata
     * @return CompletionStage que completa apÃ³s processamento
     */
    @Incoming("orders-in")
    public CompletionStage<Void> consumeWithMetadata(Message<String> message) {
        return CompletableFuture.runAsync(() -> {
            try {
                // Extrair metadata do Kafka
                IncomingKafkaRecordMetadata metadata = message
                    .getMetadata(IncomingKafkaRecordMetadata.class)
                    .orElseThrow(() -> new RuntimeException("Metadata nÃ£o disponÃ­vel"));
                
                // InformaÃ§Ãµes da mensagem
                logMetadata(metadata);
                
                // Deserializar payload
                Order order = objectMapper.readValue(message.getPayload(), Order.class);
                
                // Processar com contexto da metadata
                processOrderWithContext(order, metadata);
                
                // Acknowledge manual (commit offset)
                message.ack();
                
                LOG.infof("âœ“ Pedido %s processado | Partition: %d | Offset: %d",
                         order.getId(), metadata.getPartition(), metadata.getOffset());
                
            } catch (Exception e) {
                LOG.errorf("âœ— Erro no processamento: %s", e.getMessage());
                
                // Negative acknowledge (pode causar retry dependendo da configuraÃ§Ã£o)
                message.nack(e);
            }
        });
    }
    
    /**
     * Loga informaÃ§Ãµes de metadata da mensagem.
     */
    private void logMetadata(IncomingKafkaRecordMetadata metadata) {
        LOG.debugf("=== Metadata Kafka ===");
        LOG.debugf("Topic: %s", metadata.getTopic());
        LOG.debugf("Partition: %d", metadata.getPartition());
        LOG.debugf("Offset: %d", metadata.getOffset());
        LOG.debugf("Key: %s", metadata.getKey());
        LOG.debugf("Timestamp: %s", 
                  Instant.ofEpochMilli(metadata.getTimestamp()));
        
        // Headers
        metadata.getHeaders().forEach(header -> 
            LOG.debugf("Header: %s = %s", 
                      header.key(), 
                      new String(header.value(), StandardCharsets.UTF_8))
        );
    }
    
    /**
     * Processa pedido com informaÃ§Ãµes contextuais da metadata.
     */
    private void processOrderWithContext(Order order, IncomingKafkaRecordMetadata metadata) {
        // Extrair headers customizados
        String source = getHeaderValue(metadata, "source");
        String version = getHeaderValue(metadata, "version");
        
        LOG.infof("Processando pedido de %s (versÃ£o %s)", source, version);
        
        // LÃ³gica de negÃ³cio
        if (order.getStatus() == OrderStatus.PENDING) {
            order.setStatus(OrderStatus.CONFIRMED);
            LOG.infof("Status atualizado: %s -> CONFIRMED", order.getId());
        }
        
        // Aqui vocÃª poderia:
        // - Salvar no banco de dados
        // - Publicar evento de confirmaÃ§Ã£o
        // - Enviar notificaÃ§Ã£o
        // - Atualizar cache
    }
    
    /**
     * Extrai valor de um header especÃ­fico.
     */
    private String getHeaderValue(IncomingKafkaRecordMetadata metadata, String headerName) {
        return metadata.getHeaders().lastHeader(headerName) != null
            ? new String(metadata.getHeaders().lastHeader(headerName).value(), StandardCharsets.UTF_8)
            : "N/A";
    }
}
```

### 7.3. Consumer com Filtro e TransformaÃ§Ã£o

```java
package com.example.consumer;

import com.example.model.Order;
import com.example.model.OrderStatus;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.eclipse.microprofile.reactive.messaging.Outgoing;
import org.jboss.logging.Logger;

import java.math.BigDecimal;

/**
 * Consumer que filtra e transforma mensagens.
 * 
 * <p>Implementa padrÃ£o de filtro para processar apenas pedidos de alto valor
 * e transformÃ¡-los antes de encaminhar para outro tÃ³pico.</p>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class HighValueOrderConsumer {
    
    private static final Logger LOG = Logger.getLogger(HighValueOrderConsumer.class);
    private static final BigDecimal HIGH_VALUE_THRESHOLD = new BigDecimal("5000.00");
    
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Filtra pedidos de alto valor e os transforma.
     * 
     * <p>Pedidos com valor inferior ao threshold sÃ£o ignorados.
     * Pedidos de alto valor sÃ£o transformados e enviados para
     * o canal 'high-value-orders-out'.</p>
     * 
     * @param orderJson JSON do pedido
     * @return JSON do pedido transformado ou null se filtrado
     */
    @Incoming("orders-in")
    @Outgoing("high-value-orders-out")
    public String filterAndTransform(String orderJson) {
        try {
            Order order = objectMapper.readValue(orderJson, Order.class);
            
            // Filtrar apenas pedidos de alto valor
            if (order.getAmount().compareTo(HIGH_VALUE_THRESHOLD) < 0) {
                LOG.debugf("Pedido %s ignorado (valor baixo: %s)", 
                          order.getId(), order.getAmount());
                return null; // null = mensagem nÃ£o Ã© propagada
            }
            
            // Transformar pedido
            order.setStatus(OrderStatus.CONFIRMED);
            
            // Adicionar flag de alto valor
            LOG.infof("ğŸ”¥ Pedido de ALTO VALOR detectado: %s (R$ %s)", 
                     order.getId(), order.getAmount());
            
            // Retornar transformado
            return objectMapper.writeValueAsString(order);
            
        } catch (Exception e) {
            LOG.error("Erro ao filtrar/transformar pedido", e);
            return null;
        }
    }
}
```

### 7.4. Consumer com Retry e Dead Letter Queue

```java
package com.example.consumer;

import com.example.model.Order;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.smallrye.faulttolerance.api.CircuitBreakerName;
import io.smallrye.reactive.messaging.kafka.api.IncomingKafkaRecordMetadata;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.faulttolerance.CircuitBreaker;
import org.eclipse.microprofile.faulttolerance.Fallback;
import org.eclipse.microprofile.faulttolerance.Retry;
import org.eclipse.microprofile.reactive.messaging.Channel;
import org.eclipse.microprofile.reactive.messaging.Emitter;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.eclipse.microprofile.reactive.messaging.Message;
import org.jboss.logging.Logger;

import java.time.temporal.ChronoUnit;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;

/**
 * Consumer resiliente com retry, circuit breaker e dead letter queue.
 * 
 * <p>Implementa padrÃµes de resiliÃªncia para lidar com falhas transientes
 * e permanentes no processamento de mensagens.</p>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class ResilientOrderConsumer {
    
    private static final Logger LOG = Logger.getLogger(ResilientOrderConsumer.class);
    
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Emitter para Dead Letter Queue.
     */
    @Inject
    @Channel("orders-dlq")
    Emitter<String> dlqEmitter;
    
    /**
     * Consome mensagens com estratÃ©gia de retry.
     * 
     * <p>Tenta processar atÃ© 3 vezes antes de enviar para DLQ.
     * Usa circuit breaker para prevenir sobrecarga em caso de
     * falhas sistemÃ¡ticas.</p>
     * 
     * @param message Mensagem do Kafka
     * @return CompletionStage que completa apÃ³s processamento
     */
    @Incoming("orders-in")
    @Retry(
        maxRetries = 3,
        delay = 1000,
        delayUnit = ChronoUnit.MILLIS,
        jitter = 200
    )
    @CircuitBreaker(
        requestVolumeThreshold = 4,
        failureRatio = 0.5,
        delay = 5000,
        successThreshold = 2
    )
    @CircuitBreakerName("order-processing")
    @Fallback(fallbackMethod = "sendToDLQ")
    public CompletionStage<Void> processWithRetry(Message<String> message) {
        return CompletableFuture.runAsync(() -> {
            try {
                IncomingKafkaRecordMetadata metadata = message
                    .getMetadata(IncomingKafkaRecordMetadata.class)
                    .orElseThrow();
                
                Order order = objectMapper.readValue(message.getPayload(), Order.class);
                
                // Simular possÃ­vel falha
                processOrderWithPossibleFailure(order);
                
                message.ack();
                LOG.infof("âœ“ Pedido %s processado (tentativa bem-sucedida)", order.getId());
                
            } catch (Exception e) {
                LOG.warnf("âœ— Erro ao processar (serÃ¡ retentado): %s", e.getMessage());
                throw new RuntimeException("Falha no processamento", e);
            }
        });
    }
    
    /**
     * Fallback: envia mensagem para Dead Letter Queue.
     * 
     * <p>Este mÃ©todo Ã© chamado quando todas as tentativas de retry
     * falharam ou quando o circuit breaker estÃ¡ aberto.</p>
     */
    public CompletionStage<Void> sendToDLQ(Message<String> message) {
        return CompletableFuture.runAsync(() -> {
            try {
                Order order = objectMapper.readValue(message.getPayload(), Order.class);
                
                LOG.errorf("âŒ Pedido %s enviado para DLQ apÃ³s mÃºltiplas falhas", order.getId());
                
                // Criar mensagem de erro para DLQ
                String dlqMessage = String.format(
                    "{\"originalMessage\": %s, \"reason\": \"Max retries exceeded\", \"timestamp\": \"%s\"}",
                    message.getPayload(),
                    java.time.LocalDateTime.now()
                );
                
                dlqEmitter.send(dlqMessage);
                message.ack(); // Acknowledge para nÃ£o reprocessar
                
            } catch (Exception e) {
                LOG.fatal("Falha crÃ­tica ao enviar para DLQ", e);
                message.nack(e);
            }
        });
    }
    
    /**
     * Simula processamento que pode falhar.
     * 
     * <p>Em produÃ§Ã£o, aqui estaria a lÃ³gica real que pode falhar
     * (chamadas a APIs externas, banco de dados, etc.).</p>
     */
    private void processOrderWithPossibleFailure(Order order) {
        // Simular falha aleatÃ³ria (30% de chance)
        if (Math.random() < 0.3) {
            throw new RuntimeException("Falha transiente no processamento");
        }
        
        LOG.infof("Processando pedido: %s", order.getId());
        
        // LÃ³gica de negÃ³cio aqui...
    }
}
```

### 7.5. Consumer com Processamento em Batch

```java
package com.example.consumer;

import com.example.model.Order;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.smallrye.mutiny.Multi;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.jboss.logging.Logger;

import java.time.Duration;
import java.util.List;

/**
 * Consumer que processa mensagens em lote para maior eficiÃªncia.
 * 
 * <p>Agrupa mensagens por janela de tempo ou quantidade para
 * processamento em batch (Ãºtil para operaÃ§Ãµes em banco de dados).</p>
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class BatchOrderConsumer {
    
    private static final Logger LOG = Logger.getLogger(BatchOrderConsumer.class);
    private static final int BATCH_SIZE = 10;
    private static final Duration BATCH_TIMEOUT = Duration.ofSeconds(5);
    
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Recebe stream de mensagens.
     */
    @Incoming("orders-in")
    public Multi<Order> receiveOrders(String orderJson) {
        try {
            Order order = objectMapper.readValue(orderJson, Order.class);
            return Multi.createFrom().item(order);
        } catch (Exception e) {
            LOG.error("Erro ao deserializar pedido", e);
            return Multi.createFrom().empty();
        }
    }
    
    /**
     * Processa pedidos em batch.
     * 
     * <p>Agrupa por tamanho ou tempo, o que ocorrer primeiro.</p>
     */
    @Incoming("order-stream")
    public void processBatch(Multi<Order> orders) {
        orders
            .group().intoLists().of(BATCH_SIZE, BATCH_TIMEOUT)
            .subscribe().with(
                batch -> processBatchOfOrders(batch),
                failure -> LOG.errorf("Erro no processamento de batch: %s", failure.getMessage())
            );
    }
    
    /**
     * Processa um lote de pedidos.
     */
    private void processBatchOfOrders(List<Order> orders) {
        LOG.infof("ğŸ“¦ Processando batch de %d pedidos", orders.size());
        
        try {
            // Processar todos de uma vez (ex: INSERT em batch no banco)
            // batchInsert(orders);
            
            orders.forEach(order -> 
                LOG.debugf("  - Pedido: %s | Cliente: %s", 
                          order.getId(), order.getCustomerId())
            );
            
            LOG.infof("âœ“ Batch de %d pedidos processado com sucesso", orders.size());
            
        } catch (Exception e) {
            LOG.errorf("Erro ao processar batch: %s", e.getMessage());
            // Aqui vocÃª poderia reprocessar individualmente
        }
    }
}
```

### 7.6. Consumer com MÃºltiplos TÃ³picos

```java
package com.example.consumer;

import com.example.model.Order;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.smallrye.reactive.messaging.kafka.api.IncomingKafkaRecordMetadata;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.eclipse.microprofile.reactive.messaging.Message;
import org.jboss.logging.Logger;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;

/**
 * Consumer que escuta mÃºltiplos tÃ³picos.
 * 
 * @author Bruno Felix
 * @version 1.0
 * @since 2025-09-30
 */
@ApplicationScoped
public class MultiTopicConsumer {
    
    private static final Logger LOG = Logger.getLogger(MultiTopicConsumer.class);
    
    @Inject
    ObjectMapper objectMapper;
    
    /**
     * Consome mensagens de mÃºltiplos tÃ³picos.
     * 
     * <p>Ãštil quando a mesma lÃ³gica de processamento se aplica
     * a eventos de diferentes fontes.</p>
     */
    @Incoming("orders-in")
    @Incoming("orders-retry-in")
    public CompletionStage<Void> consumeFromMultipleTopics(Message<String> message) {
        return CompletableFuture.runAsync(() -> {
            try {
                IncomingKafkaRecordMetadata metadata = message
                    .getMetadata(IncomingKafkaRecordMetadata.class)
                    .orElseThrow();
                
                String topic = metadata.getTopic();
                Order order = objectMapper.readValue(message.getPayload(), Order.class);
                
                LOG.infof("Processando pedido %s do tÃ³pico: %s", order.getId(), topic);
                
                // LÃ³gica diferenciada por tÃ³pico
                if ("orders-retry".equals(topic)) {
                    processRetryOrder(order);
                } else {
                    processNormalOrder(order);
                }
                
                message.ack();
                
            } catch (Exception e) {
                LOG.error("Erro no processamento multi-tÃ³pico", e);
                message.nack(e);
            }
        });
    }
    
    private void processNormalOrder(Order order) {
        LOG.infof("Processamento normal: %s", order.getId());
    }
    
    private void processRetryOrder(Order order) {
        LOG.infof("Processamento de retry: %s", order.getId());
    }
}
```

---

## 8. Fluxos e Arquiteturas

## 6. Fluxo Kafka + Quarkus

```mermaid
graph TD
    A[Order REST API] -->|POST /orders| B[OrderController]
    B --> C[OrderProducer]
    C -->|Send Message| D[Kafka Topic: orders]
    
    D --> E[OrderConsumer 1<br/>Group: order-processor]
    D --> F[OrderConsumer 2<br/>Group: order-processor]
    D --> G[OrderConsumer 3<br/>Group: notification-service]
    
    E --> H[Order Processing Logic]
    F --> I[Order Processing Logic]
    G --> J[Notification Logic]
    
    H --> K[(Database)]
    I --> K
    J --> L[Email/SMS Service]
    
    subgraph "Quarkus Producer Service"
        A
        B
        C
    end
    
    subgraph "Kafka Cluster"
        D
    end
    
    subgraph "Quarkus Consumer Service 1"
        E
        F
        H
        I
    end
    
    subgraph "Quarkus Consumer Service 2"
        G
        J
    end
```

### Fluxo de Processamento Detalhado

```mermaid
sequenceDiagram
    participant Client
    participant OrderAPI
    participant Producer
    participant Kafka
    participant Consumer1
    participant Consumer2
    participant Database
    participant NotificationService
    
    Client->>OrderAPI: POST /orders
    OrderAPI->>Producer: sendOrder(order)
    Producer->>Kafka: publish to 'orders' topic
    Kafka-->>Producer: ack
    Producer-->>OrderAPI: success
    OrderAPI-->>Client: 202 Accepted
    
    Kafka->>Consumer1: deliver message (partition 0)
    Consumer1->>Database: save order
    Consumer1-->>Kafka: commit offset
    
    Kafka->>Consumer2: deliver message (partition 1)
    Consumer2->>NotificationService: send notification
    Consumer2-->>Kafka: commit offset
```

## 7. Boas PrÃ¡ticas

### SeparaÃ§Ã£o de TÃ³picos por Contexto de NegÃ³cio

```properties
# Separar por bounded context
mp.messaging.outgoing.orders-created.topic=ecommerce.orders.created
mp.messaging.outgoing.orders-updated.topic=ecommerce.orders.updated
mp.messaging.outgoing.inventory-updated.topic=inventory.stock.updated
mp.messaging.outgoing.payments-processed.topic=payment.transactions.completed
```

### ConfiguraÃ§Ã£o de Grupos de Consumidores para Escalabilidade

```properties
# Grupo para processamento de pedidos
mp.messaging.incoming.order-processing.group.id=order-processing-service
mp.messaging.incoming.order-processing.concurrency=3

# Grupo para notificaÃ§Ãµes
mp.messaging.incoming.order-notifications.group.id=notification-service
mp.messaging.incoming.order-notifications.concurrency=2

# ConfiguraÃ§Ã£o de performance
mp.messaging.incoming.order-processing.batch.size=100
mp.messaging.incoming.order-processing.fetch.min.bytes=1024
```

### Monitoramento com Quarkus Metrics

```xml
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
</dependency>
```

```java
@ApplicationScoped
public class OrderMetrics {
    
    @Inject
    MeterRegistry registry;
    
    private final Counter ordersProcessed;
    private final Timer processingTime;
    
    @PostConstruct
    public void init() {
        ordersProcessed = Counter.builder("orders.processed")
            .description("Total de pedidos processados")
            .register(registry);
            
        processingTime = Timer.builder("orders.processing.time")
            .description("Tempo de processamento de pedidos")
            .register(registry);
    }
    
    public void recordOrderProcessed() {
        ordersProcessed.increment();
    }
    
    public Timer.Sample startProcessingTimer() {
        return Timer.start(registry);
    }
}
```

### ConfiguraÃ§Ã£o de Retry e Error Handling

```properties
# ConfiguraÃ§Ã£o de retry
mp.messaging.incoming.orders-in.retry=true
mp.messaging.incoming.orders-in.retry.max-retries=3
mp.messaging.incoming.orders-in.retry.delay=1000

# Dead letter queue
mp.messaging.incoming.orders-in.failure-strategy=ignore
mp.messaging.outgoing.orders-dlq.connector=smallrye-kafka
mp.messaging.outgoing.orders-dlq.topic=orders-dlq
```

```java
@ApplicationScoped
public class OrderConsumerWithRetry {
    
    @Incoming("orders-in")
    @Retry(maxRetries = 3, delay = 1000)
    @Fallback(fallbackMethod = "fallbackProcessOrder")
    public CompletionStage<Void> processOrder(String orderJson) {
        return CompletableFuture.runAsync(() -> {
            // LÃ³gica que pode falhar
            if (Math.random() > 0.7) {
                throw new RuntimeException("Falha simulada");
            }
            System.out.println("Pedido processado com sucesso");
        });
    }
    
    public CompletionStage<Void> fallbackProcessOrder(String orderJson) {
        return CompletableFuture.runAsync(() -> {
            System.out.println("Enviando para DLQ: " + orderJson);
            // LÃ³gica para enviar para Dead Letter Queue
        });
    }
}
```

## 8. ExercÃ­cios PrÃ¡ticos

### ExercÃ­cio 1: Sistema de E-commerce com MÃºltiplos ServiÃ§os

Crie trÃªs microservices Quarkus que se comunicam via Kafka:

1. **Order Service** (Producer):
```java
@Path("/orders")
public class OrderService {
    @Inject @Channel("orders-out") Emitter<Order> orderEmitter;
    @Inject @Channel("inventory-check") Emitter<InventoryCheck> inventoryEmitter;
    
    @POST
    public Response createOrder(Order order) {
        // Verificar estoque primeiro
        InventoryCheck check = new InventoryCheck(order.productId, order.quantity);
        inventoryEmitter.send(check);
        
        // Enviar pedido
        orderEmitter.send(order);
        return Response.accepted().build();
    }
}
```

2. **Inventory Service** (Consumer/Producer):
```java
@ApplicationScoped
public class InventoryService {
    @Incoming("inventory-check")
    @Outgoing("inventory-result")
    public InventoryResult checkStock(InventoryCheck check) {
        boolean available = getCurrentStock(check.productId) >= check.quantity;
        return new InventoryResult(check.orderId, available);
    }
}
```

3. **Notification Service** (Consumer):
```java
@ApplicationScoped
public class NotificationService {
    @Incoming("orders-in")
    public void notifyOrderCreated(Order order) {
        sendEmail(order.customerEmail, "Pedido criado: " + order.id);
    }
    
    @Incoming("inventory-result")
    public void notifyInventoryStatus(InventoryResult result) {
        if (!result.available) {
            sendEmail(result.customerEmail, "Produto indisponÃ­vel");
        }
    }
}
```

### ExercÃ­cio 2: Implementar PadrÃ£o Saga com Kafka

```java
@ApplicationScoped
public class OrderSaga {
    
    @Incoming("order-created")
    @Outgoing("payment-requested")
    public PaymentRequest processOrder(OrderCreated event) {
        return new PaymentRequest(event.orderId, event.amount);
    }
    
    @Incoming("payment-completed")
    @Outgoing("shipping-requested")  
    public ShippingRequest processPayment(PaymentCompleted event) {
        return new ShippingRequest(event.orderId, event.address);
    }
    
    @Incoming("payment-failed")
    @Outgoing("order-cancelled")
    public OrderCancelled handlePaymentFailure(PaymentFailed event) {
        return new OrderCancelled(event.orderId, "Payment failed");
    }
}
```

### ExercÃ­cio 3: Sistema de Auditoria

```java
@ApplicationScoped
public class AuditService {
    
    @Incoming("orders-in")
    @Incoming("payments-in") 
    @Incoming("shipments-in")
    public CompletionStage<Void> auditEvent(Message<String> message) {
        return CompletableFuture.runAsync(() -> {
            AuditLog log = new AuditLog();
            log.timestamp = LocalDateTime.now();
            log.topic = getTopicName(message);
            log.payload = message.getPayload();
            log.partition = getPartition(message);
            log.offset = getOffset(message);
            
            saveAuditLog(log);
        });
    }
}
```

## 9. ConfiguraÃ§Ãµes AvanÃ§adas

### Exactly-Once Semantics

```properties
# Producer idempotente
mp.messaging.outgoing.orders-out.enable.idempotence=true
mp.messaging.outgoing.orders-out.retries=3
mp.messaging.outgoing.orders-out.max.in.flight.requests.per.connection=1

# TransaÃ§Ãµes
mp.messaging.outgoing.orders-out.transactional.id=orders-producer-1
```

```java
@ApplicationScoped
public class TransactionalProducer {
    
    @Inject
    @Channel("orders-out")
    Emitter<Order> emitter;
    
    @Transactional
    public void processOrderTransactionally(Order order) {
        // Salvar no banco
        orderRepository.save(order);
        
        // Enviar para Kafka (serÃ¡ commitado junto com a transaÃ§Ã£o do banco)
        emitter.send(order);
    }
}
```

### Schema Registry Integration

```xml
<dependency>
    <groupId>io.apicurio</groupId>
    <artifactId>apicurio-registry-serdes-avro-serde</artifactId>
</dependency>
```

```properties
# Schema Registry
mp.messaging.connector.smallrye-kafka.apicurio.registry.url=http://localhost:8081
mp.messaging.outgoing.orders-out.value.serializer=io.apicurio.registry.serde.avro.AvroKafkaSerializer
mp.messaging.incoming.orders-in.value.deserializer=io.apicurio.registry.serde.avro.AvroKafkaDeserializer
```

### ConfiguraÃ§Ã£o de SSL/SASL

```properties
# SSL Configuration
kafka.security.protocol=SASL_SSL
kafka.sasl.mechanism=PLAIN
kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="user" password="password";

# SSL Truststore
kafka.ssl.truststore.location=/path/to/truststore.jks
kafka.ssl.truststore.password=truststorepassword
```

## 10. Recursos Extras

### DocumentaÃ§Ã£o Oficial

- [Quarkus Kafka Guide](https://quarkus.io/guides/kafka)
- [SmallRye Reactive Messaging](https://smallrye.io/smallrye-reactive-messaging/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)

### Ferramentas de Desenvolvimento

1. **Kafka UI**: Interface web para gerenciar tÃ³picos e mensagens
2. **Conduktor**: IDE para desenvolvimento Kafka
3. **Offset Explorer**: Cliente grÃ¡fico para Kafka

### Exemplos de CÃ³digo Completos

RepositÃ³rio GitHub com exemplos prÃ¡ticos:
```bash
git clone https://github.com/quarkusio/quarkus-quickstarts
cd quarkus-quickstarts/kafka-quickstart
```

### PadrÃµes Arquiteturais

1. **Event Sourcing**: Armazenar eventos como fonte da verdade
2. **CQRS**: Separar comandos de consultas
3. **Saga Pattern**: Coordenar transaÃ§Ãµes distribuÃ­das
4. **Outbox Pattern**: Garantir consistÃªncia entre banco e mensageria

### Performance Tuning

```properties
# ConfiguraÃ§Ãµes de performance do consumer
mp.messaging.incoming.orders-in.fetch.min.bytes=50000
mp.messaging.incoming.orders-in.fetch.max.wait.ms=500
mp.messaging.incoming.orders-in.max.poll.records=500
mp.messaging.incoming.orders-in.session.timeout.ms=30000

# ConfiguraÃ§Ãµes de performance do producer
mp.messaging.outgoing.orders-out.batch.size=16384
mp.messaging.outgoing.orders-out.linger.ms=5
mp.messaging.outgoing.orders-out.compression.type=snappy
mp.messaging.outgoing.orders-out.buffer.memory=33554432
```

### Monitoramento e Observabilidade

```yaml
# Prometheus + Grafana
version: '3.8'
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

### Testes Automatizados

```java
@QuarkusTest
@TestProfile(KafkaTestProfile.class)
class OrderServiceTest {
    
    @Inject
    @Any
    InMemoryConnector connector;
    
    @Test
    void testOrderProcessing() {
        InMemorySource<Order> orders = connector.source("orders-in");
        InMemorySink<String> notifications = connector.sink("notifications-out");
        
        Order order = new Order("123", "customer1", "product1", 100.0);
        orders.send(order);
        
        await().atMost(Duration.ofSeconds(5))
               .until(() -> notifications.received().size() == 1);
        
        assertThat(notifications.received().get(0).getPayload())
               .contains("customer1");
    }
}
```

---

## 13. Recursos e ReferÃªncias

### 13.1. DocumentaÃ§Ã£o Oficial

#### Quarkus
- **Guia Oficial do Kafka:** https://quarkus.io/guides/kafka
- **Reactive Messaging:** https://quarkus.io/guides/reactive-messaging
- **Kafka Streams:** https://quarkus.io/guides/kafka-streams
- **Dev Services:** https://quarkus.io/guides/dev-services

#### SmallRye Reactive Messaging
- **DocumentaÃ§Ã£o Completa:** https://smallrye.io/smallrye-reactive-messaging/
- **Kafka Connector:** https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/3.0/kafka/kafka.html
- **Exemplos:** https://github.com/smallrye/smallrye-reactive-messaging/tree/main/examples

#### Apache Kafka
- **DocumentaÃ§Ã£o Oficial:** https://kafka.apache.org/documentation/
- **Getting Started:** https://kafka.apache.org/quickstart
- **Configuration Reference:** https://kafka.apache.org/documentation/#configuration

### 13.2. Ferramentas de Desenvolvimento

#### IDE e Plugins
- **IntelliJ IDEA Kafka Plugin:** Visualizar tÃ³picos e mensagens
- **VS Code Kafka Extension:** Gerenciamento de clusters Kafka
- **Quarkus Tools:** ExtensÃµes para desenvolvimento Quarkus

#### Ferramentas de Gerenciamento
1. **Kafka UI (Provectus)**
   - Interface web moderna
   - Gerenciamento de tÃ³picos e consumer groups
   - https://github.com/provectus/kafka-ui

2. **Conduktor**
   - IDE completa para Kafka
   - Testing e debugging avanÃ§ado
   - https://www.conduktor.io/

3. **Offset Explorer (antigo Kafka Tool)**
   - Cliente desktop gratuito
   - VisualizaÃ§Ã£o de mensagens
   - https://www.kafkatool.com/

4. **Kafdrop**
   - Interface web leve
   - VisualizaÃ§Ã£o de dados
   - https://github.com/obsidiandynamics/kafdrop

### 13.3. RepositÃ³rios de Exemplo

```bash
# Quarkus Quickstarts
git clone https://github.com/quarkusio/quarkus-quickstarts
cd quarkus-quickstarts/kafka-quickstart

# SmallRye Examples
git clone https://github.com/smallrye/smallrye-reactive-messaging
cd smallrye-reactive-messaging/examples

# Awesome Kafka
# Curadoria de recursos Kafka
https://github.com/infoslack/awesome-kafka
```

### 13.4. Livros Recomendados

- **"Kafka: The Definitive Guide"** - Neha Narkhede, Gwen Shapira, Todd Palino
- **"Designing Event-Driven Systems"** - Ben Stopford
- **"Event Streaming with Kafka Streams and ksqlDB"** - Mitch Seymour
- **"Building Microservices with Quarkus"** - Dmitry Chaban

### 13.5. Cursos Online

- **Confluent Kafka Fundamentals** (Gratuito)
- **Apache Kafka Series** - Stephane Maarek (Udemy)
- **Quarkus - Supersonic Subatomic Java** (Red Hat Learning)

### 13.6. Comunidades e Suporte

#### FÃ³runs e Grupos
- **Quarkus Zulip Chat:** https://quarkusio.zulipchat.com/
- **Stack Overflow:** Tag `quarkus` e `apache-kafka`
- **Reddit:** r/apachekafka, r/quarkus

#### GitHub
- **Quarkus Issues:** https://github.com/quarkusio/quarkus/issues
- **SmallRye Messaging:** https://github.com/smallrye/smallrye-reactive-messaging/issues

### 13.7. PadrÃµes Arquiteturais com Kafka

#### Event Sourcing
**Conceito:** Armazenar todos os eventos como fonte da verdade.

```java
@ApplicationScoped
public class EventStore {
    
    @Inject @Channel("events") 
    Emitter<Event> eventEmitter;
    
    public void storeEvent(Event event) {
        // Todos os eventos sÃ£o armazenados no Kafka
        eventEmitter.send(event);
    }
    
    @Incoming("events")
    public void projectEvent(Event event) {
        // Atualizar read models/projeÃ§Ãµes
        updateProjection(event);
    }
}
```

#### CQRS (Command Query Responsibility Segregation)
**Conceito:** Separar comandos (escrita) de consultas (leitura).

```java
// Write Side (Command)
@Path("/api/orders")
public class OrderCommandResource {
    @Inject @Channel("commands") 
    Emitter<CreateOrderCommand> commandEmitter;
    
    @POST
    public Response createOrder(CreateOrderCommand cmd) {
        commandEmitter.send(cmd);
        return Response.accepted().build();
    }
}

// Read Side (Query)
@Path("/api/orders")
public class OrderQueryResource {
    @Inject OrderQueryRepository repository;
    
    @GET
    public List<OrderView> listOrders() {
        return repository.findAll();
    }
}
```

#### Saga Pattern
**Conceito:** Coordenar transaÃ§Ãµes distribuÃ­das atravÃ©s de eventos.

```java
@ApplicationScoped
public class OrderSaga {
    
    @Incoming("order-created")
    @Outgoing("payment-requested")
    public PaymentCommand initiatePayment(OrderCreated event) {
        return new PaymentCommand(event.orderId, event.amount);
    }
    
    @Incoming("payment-completed")
    @Outgoing("shipping-requested")
    public ShipmentCommand initiateShipping(PaymentCompleted event) {
        return new ShipmentCommand(event.orderId);
    }
    
    @Incoming("payment-failed")
    @Outgoing("order-cancelled")
    public CancelOrder compensate(PaymentFailed event) {
        return new CancelOrder(event.orderId);
    }
}
```

#### Outbox Pattern
**Conceito:** Garantir consistÃªncia entre banco de dados e mensageria.

```java
@ApplicationScoped
public class OrderService {
    
    @Inject EntityManager em;
    
    @Transactional
    public void createOrder(Order order) {
        // Salvar no banco
        em.persist(order);
        
        // Salvar evento na tabela outbox
        OutboxEvent event = new OutboxEvent(
            "OrderCreated",
            objectMapper.writeValueAsString(order)
        );
        em.persist(event);
        
        // Commit atÃ´mico: tanto order quanto event sÃ£o salvos juntos
    }
}

// Processo separado lÃª da outbox e publica no Kafka
@ApplicationScoped
public class OutboxPolling {
    
    @Scheduled(every = "1s")
    public void publishEvents() {
        List<OutboxEvent> pendingEvents = outboxRepository.findPending();
        pendingEvents.forEach(event -> {
            kafkaProducer.send(event);
            outboxRepository.markAsPublished(event.id);
        });
    }
}
```

### 13.8. Performance Tuning - Checklist

#### Producer
- [ ] Habilitar compressÃ£o (`compression.type=snappy`)
- [ ] Configurar batch size adequado (`batch.size=16384`)
- [ ] Ajustar linger.ms (`linger.ms=5-10`)
- [ ] Habilitar idempotÃªncia (`enable.idempotence=true`)
- [ ] Configurar acks (`acks=all` para durabilidade)
- [ ] Tunar buffer memory (`buffer.memory=33554432`)

#### Consumer
- [ ] Ajustar fetch.min.bytes (`fetch.min.bytes=1024`)
- [ ] Configurar max.poll.records (`max.poll.records=500`)
- [ ] Tunar session.timeout.ms (`session.timeout.ms=30000`)
- [ ] Configurar concorrÃªncia (`mp.messaging.incoming.*.concurrency=3`)
- [ ] Usar processamento em batch quando possÃ­vel
- [ ] Implementar health checks

#### Broker/Cluster
- [ ] Configurar replication factor adequado (â‰¥3 em produÃ§Ã£o)
- [ ] Ajustar num.partitions baseado na carga
- [ ] Configurar log.segment.bytes e log.retention.bytes
- [ ] Monitorar disk I/O e network throughput
- [ ] Habilitar log compaction se necessÃ¡rio
- [ ] Configurar min.insync.replicas

### 13.9. Troubleshooting Comum

#### Problema: Consumer Lag Alto
**Sintomas:** Offset nÃ£o acompanha a produÃ§Ã£o.

**SoluÃ§Ãµes:**
```properties
# Aumentar paralelismo
mp.messaging.incoming.orders-in.concurrency=5

# Aumentar fetch size
mp.messaging.incoming.orders-in.fetch.min.bytes=50000
mp.messaging.incoming.orders-in.max.poll.records=1000

# Adicionar mais partiÃ§Ãµes ao tÃ³pico
# Escalar horizontalmente (mais instances do consumer)
```

#### Problema: Mensagens Duplicadas
**Sintomas:** Mesma mensagem processada mÃºltiplas vezes.

**SoluÃ§Ãµes:**
```properties
# Habilitar idempotÃªncia no producer
mp.messaging.outgoing.orders-out.enable.idempotence=true

# Implementar idempotÃªncia na aplicaÃ§Ã£o
@Incoming("orders-in")
public void processIdempotent(Order order) {
    if (processedOrders.contains(order.getId())) {
        LOG.warn("Pedido jÃ¡ processado, ignorando");
        return;
    }
    process(order);
    processedOrders.add(order.getId());
}
```

#### Problema: Timeout de ConexÃ£o
**Sintomas:** `TimeoutException` ao produzir/consumir.

**SoluÃ§Ãµes:**
```properties
# Aumentar timeouts
kafka.connections.max.idle.ms=600000
kafka.request.timeout.ms=30000
mp.messaging.outgoing.orders-out.max.block.ms=10000
```

#### Problema: Rebalance Frequente
**Sintomas:** Logs mostrando "Revoke" e "Assign" constantemente.

**SoluÃ§Ãµes:**
```properties
# Aumentar session timeout
mp.messaging.incoming.orders-in.session.timeout.ms=45000
mp.messaging.incoming.orders-in.heartbeat.interval.ms=3000

# Reduzir max.poll.records se processamento Ã© lento
mp.messaging.incoming.orders-in.max.poll.records=100
```

---

## 14. ConclusÃ£o

### RecapitulaÃ§Ã£o

Neste guia completo, vocÃª aprendeu:

âœ… **Fundamentos do Kafka**
- Arquitetura e componentes principais
- Conceitos de tÃ³picos, partiÃ§Ãµes e consumer groups
- Garantias de entrega e retenÃ§Ã£o de dados

âœ… **IntegraÃ§Ã£o com Quarkus**
- Setup e configuraÃ§Ã£o de projetos
- SmallRye Reactive Messaging
- Produtores e consumidores reativos

âœ… **PadrÃµes e Boas PrÃ¡ticas**
- Event Sourcing, CQRS e Saga
- Retry, Circuit Breaker e DLQ
- Performance tuning e monitoramento

âœ… **ProduÃ§Ã£o Ready**
- ConfiguraÃ§Ãµes avanÃ§adas de seguranÃ§a
- Testes automatizados
- Observabilidade e troubleshooting

### PrÃ³ximos Passos

#### NÃ­vel Iniciante â†’ IntermediÃ¡rio
1. **Implemente os exercÃ­cios prÃ¡ticos** da seÃ§Ã£o 12
2. **Configure um cluster Kafka real** (nÃ£o Docker)
3. **Adicione Schema Registry** para validaÃ§Ã£o de schemas
4. **Implemente testes de integraÃ§Ã£o** completos

#### NÃ­vel IntermediÃ¡rio â†’ AvanÃ§ado
1. **Explore Kafka Streams** para processamento complexo
2. **Implemente padrÃ£o Saga** em um sistema real
3. **Configure Kafka em Kubernetes** com operadores
4. **Adicione observabilidade** com Grafana + Prometheus
5. **Estude ksqlDB** para SQL sobre streams

#### Recursos para Continuar Aprendendo
- [ ] CertificaÃ§Ã£o **Confluent Certified Developer**
- [ ] Implementar **CDC com Debezium + Kafka**
- [ ] Estudar **Kafka Connect** para integraÃ§Ã£o com bancos
- [ ] Explorar **Apache Flink** para stream processing avanÃ§ado

### Filosofia de Desenvolvimento

> **"Kafka + Quarkus = Microservices SupersÃ´nicos"**

A combinaÃ§Ã£o de Kafka e Quarkus oferece:
- âš¡ **Performance excepcional** para aplicaÃ§Ãµes cloud-native
- ğŸ”„ **Reatividade nativa** para escalabilidade
- ğŸ¯ **Developer experience superior** com live coding
- ğŸ³ **Container-friendly** com footprint mÃ­nimo
- ğŸ“Š **Observabilidade built-in** para produÃ§Ã£o

### Checklist Final para ProduÃ§Ã£o

Antes de colocar sua aplicaÃ§Ã£o Kafka + Quarkus em produÃ§Ã£o:

#### Funcionalidade
- [ ] Produtores enviam mensagens com chave apropriada
- [ ] Consumidores processam idempotentemente
- [ ] Dead Letter Queue configurada
- [ ] Retry com backoff exponencial
- [ ] Circuit breaker implementado

#### Performance
- [ ] Load testing realizado
- [ ] LatÃªncia P99 < 100ms
- [ ] Throughput atende SLA
- [ ] Consumer lag monitorado
- [ ] PartiÃ§Ãµes dimensionadas corretamente

#### Confiabilidade
- [ ] Replication factor â‰¥ 3
- [ ] Min insync replicas = 2
- [ ] Acks = all para dados crÃ­ticos
- [ ] Backup e disaster recovery planejados
- [ ] Runbooks documentados

#### SeguranÃ§a
- [ ] SSL/TLS habilitado
- [ ] SASL autenticaÃ§Ã£o configurada
- [ ] ACLs definidas
- [ ] Secrets em vault (nÃ£o hardcoded)
- [ ] Auditoria habilitada

#### Observabilidade
- [ ] MÃ©tricas exportadas para Prometheus
- [ ] Dashboards no Grafana configurados
- [ ] Alertas crÃ­ticos definidos
- [ ] Logs estruturados (JSON)
- [ ] Distributed tracing (OpenTelemetry)

#### DocumentaÃ§Ã£o
- [ ] Arquitetura documentada
- [ ] APIs documentadas (OpenAPI)
- [ ] Runbooks atualizados
- [ ] Diagramas de fluxo criados
- [ ] Knowledge base mantida

---

## ApÃªndice A: GlossÃ¡rio

| Termo | DefiniÃ§Ã£o |
|-------|-----------|
| **Broker** | Servidor Kafka que armazena e serve dados |
| **Topic** | Categoria lÃ³gica onde mensagens sÃ£o publicadas |
| **Partition** | SubdivisÃ£o de um tÃ³pico para paralelizaÃ§Ã£o |
| **Offset** | Identificador Ãºnico sequencial de mensagem |
| **Consumer Group** | Grupo de consumidores que compartilham o trabalho |
| **Replication** | CÃ³pias de partiÃ§Ãµes em mÃºltiplos brokers |
| **Leader** | Broker responsÃ¡vel por reads/writes de uma partiÃ§Ã£o |
| **Replica** | CÃ³pia backup de uma partiÃ§Ã£o |
| **Producer** | AplicaÃ§Ã£o que publica mensagens no Kafka |
| **Consumer** | AplicaÃ§Ã£o que lÃª mensagens do Kafka |
| **Commit** | Persistir offset de mensagem processada |
| **Lag** | DiferenÃ§a entre Ãºltimo offset e offset atual do consumer |
| **Retention** | Tempo/tamanho que mensagens sÃ£o mantidas |
| **Compaction** | Manter apenas Ãºltima versÃ£o de cada chave |

---

## ApÃªndice B: Comandos Ãšteis (ReferÃªncia RÃ¡pida)

### Gerenciamento de TÃ³picos

```powershell
# Criar tÃ³pico
docker exec kafka-broker kafka-topics --create --topic my-topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

# Listar tÃ³picos
docker exec kafka-broker kafka-topics --list --bootstrap-server localhost:9092

# Descrever tÃ³pico
docker exec kafka-broker kafka-topics --describe --topic my-topic --bootstrap-server localhost:9092

# Deletar tÃ³pico
docker exec kafka-broker kafka-topics --delete --topic my-topic --bootstrap-server localhost:9092

# Aumentar partiÃ§Ãµes
docker exec kafka-broker kafka-topics --alter --topic my-topic --partitions 5 --bootstrap-server localhost:9092
```

### Consumer Groups

```powershell
# Listar grupos
docker exec kafka-broker kafka-consumer-groups --list --bootstrap-server localhost:9092

# Descrever grupo
docker exec kafka-broker kafka-consumer-groups --describe --group my-group --bootstrap-server localhost:9092

# Reset offset para inÃ­cio
docker exec kafka-broker kafka-consumer-groups --reset-offsets --group my-group --topic my-topic --to-earliest --execute --bootstrap-server localhost:9092

# Reset offset para timestamp
docker exec kafka-broker kafka-consumer-groups --reset-offsets --group my-group --topic my-topic --to-datetime 2025-09-30T10:00:00.000 --execute --bootstrap-server localhost:9092
```

### ProduÃ§Ã£o/Consumo

```powershell
# Produzir mensagens
docker exec -it kafka-broker kafka-console-producer --topic my-topic --bootstrap-server localhost:9092

# Consumir do inÃ­cio
docker exec -it kafka-broker kafka-console-consumer --topic my-topic --from-beginning --bootstrap-server localhost:9092

# Consumir com chave
docker exec -it kafka-broker kafka-console-consumer --topic my-topic --from-beginning --property print.key=true --bootstrap-server localhost:9092
```

### Quarkus

```powershell
# Dev mode
./mvnw quarkus:dev

# Build nativo
./mvnw package -Dnative

# Build container
./mvnw package -Dquarkus.container-image.build=true

# Adicionar extensÃ£o
./mvnw quarkus:add-extension -Dextensions="kafka"
```

---

**ğŸ‰ ParabÃ©ns!** VocÃª completou o guia completo de Kafka com Quarkus. Agora vocÃª estÃ¡ pronto para construir arquiteturas de microservices robustas, escalÃ¡veis e reativas!

**Desenvolvido com â¤ï¸ por Bruno Felix**  
**VersÃ£o:** 2.0  
**Data:** 30 de Setembro de 2025  
**LicenÃ§a:** MIT

---

Com este guia completo, vocÃª tem todas as ferramentas, conhecimentos e melhores prÃ¡ticas necessÃ¡rias para implementar uma arquitetura robusta de microservices usando Apache Kafka e Quarkus, desde conceitos fundamentais atÃ© configuraÃ§Ãµes avanÃ§adas prontas para produÃ§Ã£o.