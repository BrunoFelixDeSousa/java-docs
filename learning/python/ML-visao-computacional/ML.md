[# ‚¨Ö Voltar para o √≠ndice principal](../README.md)

## üìä Ci√™ncia de Dados e Machine Learning

| Biblioteca                             | Descri√ß√£o T√©cnica                                                                  |
| -------------------------------------- | ---------------------------------------------------------------------------------- |
| [NumPy](./libs/NumPy.md)               | Manipula√ß√£o eficiente de arrays e opera√ß√µes matem√°ticas vetorizadas.               |
| [Pandas](./libs/Pandas.md)             | Manipula√ß√£o e an√°lise de dados tabulares com estrutura DataFrame.                  |
| Matplotlib                             | Cria√ß√£o de gr√°ficos est√°ticos (linha, barra, dispers√£o etc.).                      |
| Seaborn                                | Visualiza√ß√µes estat√≠sticas elegantes e simplificadas com base no Matplotlib.       |
| [Scikit-learn](./libs/Scikit-learn.md) | Conjunto de algoritmos de machine learning (regress√£o, classifica√ß√£o, clustering). |
| TensorFlow                             | Framework para cria√ß√£o e treino de modelos de Deep Learning (by Google).           |
| PyTorch                                | Framework de Deep Learning com execu√ß√£o din√¢mica de grafos (by Meta).              |
| XGBoost                                | Algoritmo eficiente de boosting para tarefas supervisionadas (ranking, etc).       |

## üß† IA, NLP e Vis√£o Computacional

| Biblioteca    | Descri√ß√£o T√©cnica                                                        |
| ------------- | ------------------------------------------------------------------------ |
| spaCy         | Processamento de linguagem natural com performance de produ√ß√£o.          |
| nltk          | Toolkit educacional para NLP com algoritmos cl√°ssicos e corpus embutido. |
| transformers  | Modelos SOTA de NLP como BERT, GPT, T5 (via Hugging Face).               |
| opencv-python | Manipula√ß√£o de imagens e v√≠deos para vis√£o computacional.                |

# üìä Guia Completo de Algoritmos de Machine Learning

## üîç Classifica√ß√£o

Classifica√ß√£o √© usada quando a sa√≠da esperada √© uma categoria (ex: "spam" ou "n√£o spam", "gato" ou "cachorro", "aprovado" ou "reprovado").

| Algoritmo                        | Descri√ß√£o                                                                                          | Vantagens                                                                                                                                            | Desvantagens                                                                                                                                                               | Aplica√ß√µes                                                                                                | Complexidade                         |
| -------------------------------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------ |
| **K-Nearest Neighbors (KNN)**    | Classifica com base nos k vizinhos mais pr√≥ximos, usando dist√¢ncias como Euclidiana ou Manhattan   | ‚Ä¢ Simples e intuitivo<br>‚Ä¢ N√£o faz suposi√ß√µes sobre dados<br>‚Ä¢ Eficaz para dados pequenos<br>‚Ä¢ Adapt√°vel pela escolha de k                           | ‚Ä¢ Computacionalmente caro para grandes datasets<br>‚Ä¢ Sens√≠vel a atributos irrelevantes<br>‚Ä¢ Requer normaliza√ß√£o<br>‚Ä¢ Problema com dimensionalidade alta                    | ‚Ä¢ Sistemas de recomenda√ß√£o<br>‚Ä¢ An√°lise de cr√©dito<br>‚Ä¢ Reconhecimento de padr√µes<br>‚Ä¢ Diagn√≥stico m√©dico | Treinamento: O(1)<br>Predi√ß√£o: O(nd) |
| **Support Vector Machine (SVM)** | Encontra hiperplano √≥timo para separa√ß√£o entre classes, maximizando a margem                       | ‚Ä¢ Eficaz em espa√ßos de alta dimens√£o<br>‚Ä¢ Usa fun√ß√µes kernel para problemas n√£o-lineares<br>‚Ä¢ Boa generaliza√ß√£o<br>‚Ä¢ Robusto contra overfitting      | ‚Ä¢ Dif√≠cil interpreta√ß√£o<br>‚Ä¢ Lento em grandes datasets<br>‚Ä¢ Sens√≠vel a escolha de kernel e par√¢metros<br>‚Ä¢ N√£o estima probabilidades diretamente                           | ‚Ä¢ Reconhecimento de imagem<br>‚Ä¢ Classifica√ß√£o de texto<br>‚Ä¢ Detec√ß√£o de fraudes<br>‚Ä¢ Bioinform√°tica       | O(n¬≤d) a O(n¬≥d)                      |
| **Decision Tree**                | Usa perguntas bin√°rias para formar uma √°rvore de decis√£o hier√°rquica                               | ‚Ä¢ Altamente interpret√°vel<br>‚Ä¢ Requer pouco pr√©-processamento<br>‚Ä¢ N√£o sens√≠vel a outliers<br>‚Ä¢ Lida com dados num√©ricos e categ√≥ricos               | ‚Ä¢ Propenso a overfitting<br>‚Ä¢ Inst√°vel (pequenas varia√ß√µes nos dados mudam muito a √°rvore)<br>‚Ä¢ Pode criar √°rvores complexas<br>‚Ä¢ Tend√™ncia a vi√©s em classes dominantes   | ‚Ä¢ Diagn√≥stico m√©dico<br>‚Ä¢ An√°lise de risco<br>‚Ä¢ Sistemas de aprova√ß√£o<br>‚Ä¢ Detec√ß√£o de falhas             | O(n √ó d √ó log(n))                    |
| **Random Forest**                | Conjunto (ensemble) de √°rvores de decis√£o treinadas com bootstrap e sele√ß√£o aleat√≥ria de atributos | ‚Ä¢ Robusto contra overfitting<br>‚Ä¢ Alta precis√£o<br>‚Ä¢ Estima import√¢ncia dos atributos<br>‚Ä¢ Paraleliz√°vel                                             | ‚Ä¢ Menos interpret√°vel que √°rvores simples<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Tend√™ncia a vi√©s em atributos cont√≠nuos<br>‚Ä¢ Lento para predi√ß√£o com muitas √°rvores       | ‚Ä¢ Finan√ßas<br>‚Ä¢ Medicina<br>‚Ä¢ Ecologia<br>‚Ä¢ E-commerce                                                    | O(n_trees √ó n √ó d √ó log(n))          |
| **Naive Bayes**                  | Usa probabilidade condicional com suposi√ß√£o de independ√™ncia entre atributos                       | ‚Ä¢ Muito r√°pido<br>‚Ä¢ Eficiente com poucos dados<br>‚Ä¢ Escal√°vel para grandes datasets<br>‚Ä¢ Bom para dados de alta dimens√£o                             | ‚Ä¢ Suposi√ß√£o de independ√™ncia irrealista<br>‚Ä¢ Performance limitada para rela√ß√µes complexas<br>‚Ä¢ Problema com atributos com valor zero<br>‚Ä¢ Sens√≠vel a atributos redundantes | ‚Ä¢ Filtro de spam<br>‚Ä¢ Classifica√ß√£o de texto<br>‚Ä¢ An√°lise de sentimento<br>‚Ä¢ Diagn√≥sticos simples         | O(nd)                                |
| **Logistic Regression**          | Usa fun√ß√£o sigmoide para prever probabilidade de classe, estabelecendo um limite de decis√£o linear | ‚Ä¢ Probabilidades interpret√°veis<br>‚Ä¢ R√°pido e eficiente<br>‚Ä¢ F√°cil implementa√ß√£o<br>‚Ä¢ Baixo risco de overfitting                                     | ‚Ä¢ Limitado a rela√ß√µes lineares<br>‚Ä¢ Assume fronteiras lineares<br>‚Ä¢ Sens√≠vel a outliers<br>‚Ä¢ N√£o ideal para rela√ß√µes complexas                                             | ‚Ä¢ Marketing (convers√£o)<br>‚Ä¢ Admiss√£o educacional<br>‚Ä¢ Previs√£o de risco<br>‚Ä¢ Diagn√≥stico m√©dico          | O(nd)                                |
| **Gradient Boosting**            | Constr√≥i modelos sequencialmente, cada um corrigindo erros do anterior                             | ‚Ä¢ Alta precis√£o<br>‚Ä¢ Flex√≠vel para diferentes fun√ß√µes de perda<br>‚Ä¢ Lida bem com dados heterog√™neos<br>‚Ä¢ Bom para dados desbalanceados               | ‚Ä¢ Sens√≠vel a outliers e ru√≠do<br>‚Ä¢ Risco de overfitting<br>‚Ä¢ Treinamento sequencial (n√£o paraleliz√°vel)<br>‚Ä¢ Mais par√¢metros para ajustar                                  | ‚Ä¢ Competi√ß√µes de ML<br>‚Ä¢ Detec√ß√£o de fraudes<br>‚Ä¢ Previs√£o de riscos<br>‚Ä¢ Ranqueamento web                | O(n_trees √ó n √ó d)                   |
| **XGBoost**                      | Implementa√ß√£o otimizada de Gradient Boosting com regulariza√ß√£o                                     | ‚Ä¢ Performance superior<br>‚Ä¢ Paraleliza√ß√£o eficiente<br>‚Ä¢ Regulariza√ß√£o integrada<br>‚Ä¢ Lida com valores ausentes                                      | ‚Ä¢ Requer ajuste fino de par√¢metros<br>‚Ä¢ Pode ser excessivamente complexo<br>‚Ä¢ Treinamento demorado<br>‚Ä¢ Dif√≠cil interpreta√ß√£o                                              | ‚Ä¢ Competi√ß√µes Kaggle<br>‚Ä¢ Sistemas financeiros<br>‚Ä¢ Ranqueamento<br>‚Ä¢ Previs√£o de CTR                     | O(n_trees √ó n √ó d √ó log(n))          |
| **Neural Networks**              | Redes de neur√¥nios artificiais conectados em camadas, inspiradas no c√©rebro humano                 | ‚Ä¢ Captura rela√ß√µes complexas n√£o-lineares<br>‚Ä¢ Adapt√°vel a diversos tipos de dados<br>‚Ä¢ Aprende representa√ß√µes hier√°rquicas<br>‚Ä¢ Escal√°vel com dados | ‚Ä¢ Caixa preta (baixa interpretabilidade)<br>‚Ä¢ Requer muitos dados<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Propenso a overfitting sem regulariza√ß√£o                          | ‚Ä¢ Vis√£o computacional<br>‚Ä¢ Processamento de linguagem<br>‚Ä¢ Reconhecimento de voz<br>‚Ä¢ Sistemas complexos  | O(n √ó e √ó h √ó o)                     |
| **LightGBM**                     | Framework de gradient boosting otimizado para efici√™ncia                                           | ‚Ä¢ Extremamente r√°pido<br>‚Ä¢ Baixo consumo de mem√≥ria<br>‚Ä¢ Alta precis√£o<br>‚Ä¢ Suporte a dados categ√≥ricos                                              | ‚Ä¢ Sens√≠vel a overfitting em dados pequenos<br>‚Ä¢ Menos precis√£o com par√¢metros default<br>‚Ä¢ Complexidade de configura√ß√£o<br>‚Ä¢ Menos estudo te√≥rico                          | ‚Ä¢ Big Data<br>‚Ä¢ CTR (Click-Through Rate)<br>‚Ä¢ Ranqueamento<br>‚Ä¢ Sistemas em tempo real                    | O(n_leaves √ó n)                      |

## üìà Regress√£o

Regress√£o √© usada quando a sa√≠da esperada √© num√©rica cont√≠nua (ex: pre√ßo de uma casa, temperatura, vendas).

| Algoritmo                           | Descri√ß√£o                                                                            | Vantagens                                                                                                                                           | Desvantagens                                                                                                                        | Aplica√ß√µes                                                                                                | Complexidade                |
| ----------------------------------- | ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | --------------------------- |
| **Linear Regression**               | Ajusta uma linha reta para prever valor cont√≠nuo, minimizando erro quadr√°tico        | ‚Ä¢ Interpretabilidade direta<br>‚Ä¢ Computacionalmente eficiente<br>‚Ä¢ Base para muitos m√©todos<br>‚Ä¢ Probabilisticamente fundamentado                   | ‚Ä¢ Limitado a rela√ß√µes lineares<br>‚Ä¢ Sens√≠vel a outliers<br>‚Ä¢ Assume independ√™ncia dos erros<br>‚Ä¢ Requer homocedasticidade           | ‚Ä¢ Previs√£o de vendas<br>‚Ä¢ An√°lise econ√¥mica<br>‚Ä¢ Epidemiologia<br>‚Ä¢ Ci√™ncias sociais                      | O(nd¬≤)                      |
| **Polynomial Regression**           | Estende a regress√£o linear usando termos polinomiais para modelar curvas             | ‚Ä¢ Captura rela√ß√µes n√£o-lineares<br>‚Ä¢ Baseado em regress√£o linear<br>‚Ä¢ Interpret√°vel<br>‚Ä¢ Flex√≠vel em grau                                           | ‚Ä¢ Alto risco de overfitting<br>‚Ä¢ Sens√≠vel a outliers<br>‚Ä¢ Inst√°vel para extrapola√ß√£o<br>‚Ä¢ Escolha adequada do grau                  | ‚Ä¢ An√°lise de crescimento<br>‚Ä¢ Tend√™ncias econ√¥micas<br>‚Ä¢ Rela√ß√µes f√≠sicas<br>‚Ä¢ Biologia                   | O(nd¬≤)                      |
| **Ridge Regression**                | Adiciona penalidade L2 aos coeficientes para controlar sua magnitude                 | ‚Ä¢ Reduz overfitting<br>‚Ä¢ Estabiliza coeficientes<br>‚Ä¢ Lida com multicolinearidade<br>‚Ä¢ N√£o descarta vari√°veis                                       | ‚Ä¢ Ainda assume linearidade<br>‚Ä¢ N√£o realiza sele√ß√£o de vari√°veis<br>‚Ä¢ Requer ajuste de Œª<br>‚Ä¢ Menos interpret√°vel                   | ‚Ä¢ Previs√£o financeira<br>‚Ä¢ Dados com multicolinearidade<br>‚Ä¢ Gen√¥mica<br>‚Ä¢ Neuroci√™ncia                   | O(nd¬≤)                      |
| **Lasso Regression**                | Adiciona penalidade L1 aos coeficientes, induzindo esparsidade                       | ‚Ä¢ Reduz overfitting<br>‚Ä¢ Seleciona vari√°veis importantes<br>‚Ä¢ Elimina atributos irrelevantes<br>‚Ä¢ Cria modelos simples                              | ‚Ä¢ Inst√°vel com atributos correlacionados<br>‚Ä¢ Requer ajuste de Œª<br>‚Ä¢ Menos precis√£o em alguns casos<br>‚Ä¢ Mais complexo que Ridge   | ‚Ä¢ Sele√ß√£o de atributos<br>‚Ä¢ Gen√¥mica<br>‚Ä¢ Economia<br>‚Ä¢ Modelos de previs√£o esparsos                      | O(nd¬≤)                      |
| **Elastic Net**                     | Combina penalidades L1 e L2 para balancear Ridge e Lasso                             | ‚Ä¢ Lida com correla√ß√µes entre atributos<br>‚Ä¢ Seleciona grupos correlacionados<br>‚Ä¢ Estabilidade em dados esparsos<br>‚Ä¢ Equilibra vi√©s-vari√¢ncia      | ‚Ä¢ Dois hiperpar√¢metros para ajustar<br>‚Ä¢ Mais complexo computacionalmente<br>‚Ä¢ Menos interpret√°vel<br>‚Ä¢ Requer valida√ß√£o cruzada    | ‚Ä¢ An√°lise de dados gen√¥micos<br>‚Ä¢ Processamento de imagens<br>‚Ä¢ Finan√ßas<br>‚Ä¢ Modelos preditivos robustos | O(nd¬≤)                      |
| **Decision Tree Regressor**         | Divide recursivamente o espa√ßo de atributos predizendo valores em cada folha         | ‚Ä¢ Captura n√£o-linearidades<br>‚Ä¢ F√°cil visualiza√ß√£o<br>‚Ä¢ N√£o requer escalonamento<br>‚Ä¢ Lida bem com intera√ß√µes                                       | ‚Ä¢ Tend√™ncia a overfitting<br>‚Ä¢ Inst√°vel (vari√¢ncia alta)<br>‚Ä¢ N√£o suave nas predi√ß√µes<br>‚Ä¢ Limitado em extrapola√ß√£o                 | ‚Ä¢ Previs√£o de demanda<br>‚Ä¢ Ecologia<br>‚Ä¢ Avalia√ß√µes imobili√°rias<br>‚Ä¢ Sistemas de recomenda√ß√£o            | O(n √ó d √ó log(n))           |
| **Random Forest Regressor**         | Ensemble de √°rvores de regress√£o com bootstrap e sele√ß√£o rand√¥mica de atributos      | ‚Ä¢ Robusto a outliers<br>‚Ä¢ Estima incerteza<br>‚Ä¢ Evita overfitting<br>‚Ä¢ Captura intera√ß√µes complexas                                                 | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Dif√≠cil interpreta√ß√£o<br>‚Ä¢ N√£o extrapola bem<br>‚Ä¢ Vi√©s para atributos com muitos n√≠veis         | ‚Ä¢ Previs√£o de pre√ßos<br>‚Ä¢ An√°lise ambiental<br>‚Ä¢ Meteorologia<br>‚Ä¢ Finan√ßas                               | O(n_trees √ó n √ó d √ó log(n)) |
| **SVR (Support Vector Regression)** | Adapta SVM para regress√£o, criando um tubo de toler√¢ncia                             | ‚Ä¢ Robusto a outliers<br>‚Ä¢ Eficaz em espa√ßos dimensionais altos<br>‚Ä¢ Lida com n√£o-linearidades via kernels<br>‚Ä¢ Generaliza bem                       | ‚Ä¢ Treinamento lento<br>‚Ä¢ Dif√≠cil interpreta√ß√£o<br>‚Ä¢ Sens√≠vel a par√¢metros<br>‚Ä¢ Mem√≥ria intensiva para grandes datasets              | ‚Ä¢ Previs√£o financeira<br>‚Ä¢ Demanda energ√©tica<br>‚Ä¢ Propriedades qu√≠micas<br>‚Ä¢ Processamento de sinais     | O(n¬≤d) a O(n¬≥d)             |
| **Gradient Boosting Regressor**     | Constr√≥i regressores fracos em sequ√™ncia, cada um melhorando o anterior              | ‚Ä¢ Alta precis√£o<br>‚Ä¢ Robusto a outliers e dados faltantes<br>‚Ä¢ Captura rela√ß√µes complexas<br>‚Ä¢ Flex√≠vel                                             | ‚Ä¢ Alto custo computacional<br>‚Ä¢ Muitos hiperpar√¢metros<br>‚Ä¢ Propenso a overfitting<br>‚Ä¢ Dif√≠cil interpreta√ß√£o                       | ‚Ä¢ Previs√£o de pre√ßos<br>‚Ä¢ Finan√ßas<br>‚Ä¢ Meteorologia<br>‚Ä¢ Medicina                                        | O(n_trees √ó n √ó d)          |
| **Neural Network Regressor**        | Redes neurais aplicadas para prever valores cont√≠nuos                                | ‚Ä¢ Modela rela√ß√µes altamente complexas<br>‚Ä¢ Aprende representa√ß√µes hier√°rquicas<br>‚Ä¢ Flex√≠vel e adapt√°vel<br>‚Ä¢ Bom para grandes volumes de dados     | ‚Ä¢ Requer muitos dados<br>‚Ä¢ Caixa preta (baixa interpretabilidade)<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Sens√≠vel a hiperpar√¢metros | ‚Ä¢ Previs√£o financeira<br>‚Ä¢ Sistemas din√¢micos<br>‚Ä¢ F√≠sica computacional<br>‚Ä¢ Processamento de sinais      | O(n √ó e √ó h √ó o)            |
| **ARIMA**                           | Modelos estat√≠sticos para s√©ries temporais baseados em autoregress√£o e m√©dias m√≥veis | ‚Ä¢ Espec√≠fico para s√©ries temporais<br>‚Ä¢ Captura tend√™ncias e sazonalidade<br>‚Ä¢ Bem fundamentado estatisticamente<br>‚Ä¢ Boas propriedades de previs√£o | ‚Ä¢ Limitado a padr√µes lineares<br>‚Ä¢ Assume estacionariedade<br>‚Ä¢ Requer an√°lise manual<br>‚Ä¢ N√£o escala bem                           | ‚Ä¢ Previs√µes econ√¥micas<br>‚Ä¢ Meteorologia<br>‚Ä¢ Vendas<br>‚Ä¢ Finan√ßas                                        | O(nm¬≤)                      |

## üß† Agrupamento (Clustering)

Agrupamento √© usado para descobrir padr√µes e estruturas em dados n√£o rotulados (aprendizado n√£o supervisionado).

| Algoritmo                   | Descri√ß√£o                                                                            | Vantagens                                                                                                                                           | Desvantagens                                                                                                                                                             | Aplica√ß√µes                                                                                                       | Complexidade |
| --------------------------- | ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------- | ------------ |
| **K-Means**                 | Agrupa dados minimizando a dist√¢ncia aos centroides, iterativamente                  | ‚Ä¢ Simples e intuitivo<br>‚Ä¢ Escal√°vel e eficiente<br>‚Ä¢ Resultados f√°ceis de interpretar<br>‚Ä¢ Converg√™ncia garantida                                  | ‚Ä¢ Sens√≠vel √† inicializa√ß√£o<br>‚Ä¢ Requer defini√ß√£o pr√©via de k<br>‚Ä¢ Sup√µe clusters convexos e isotr√≥picos<br>‚Ä¢ Sens√≠vel a outliers                                         | ‚Ä¢ Segmenta√ß√£o de clientes<br>‚Ä¢ Compress√£o de imagens<br>‚Ä¢ Organiza√ß√£o de documentos<br>‚Ä¢ Agrupamento de produtos | O(nkdi)      |
| **DBSCAN**                  | Agrupa baseado na densidade de pontos, identificando √°reas de alta densidade         | ‚Ä¢ N√£o requer n√∫mero de clusters<br>‚Ä¢ Detecta clusters de formato arbitr√°rio<br>‚Ä¢ Identifica outliers<br>‚Ä¢ Robusto a ru√≠dos                          | ‚Ä¢ Dificuldade com clusters de densidades variadas<br>‚Ä¢ Sens√≠vel aos par√¢metros eps e minPts<br>‚Ä¢ N√£o funciona bem em alta dimensionalidade<br>‚Ä¢ Custo computacional alto | ‚Ä¢ An√°lise espacial<br>‚Ä¢ Detec√ß√£o de anomalias<br>‚Ä¢ Segmenta√ß√£o de imagens<br>‚Ä¢ An√°lise de redes sociais          | O(n¬≤)        |
| **Hierarchical Clustering** | Cria hierarquia de clusters por aglomera√ß√£o (bottom-up) ou divis√£o (top-down)        | ‚Ä¢ N√£o requer n√∫mero pr√©-definido de clusters<br>‚Ä¢ Cria dendrograma visual<br>‚Ä¢ Captura estrutura hier√°rquica<br>‚Ä¢ Flex√≠vel em m√©tricas de dist√¢ncia | ‚Ä¢ Alto custo computacional<br>‚Ä¢ Sens√≠vel a outliers<br>‚Ä¢ Decis√µes de merge/split s√£o permanentes<br>‚Ä¢ Dif√≠cil com grandes datasets                                       | ‚Ä¢ Biologia (taxonomia)<br>‚Ä¢ An√°lise de documentos<br>‚Ä¢ Segmenta√ß√£o de clientes<br>‚Ä¢ Organiza√ß√£o de conhecimento  | O(n¬≥)        |
| **OPTICS**                  | Extens√£o do DBSCAN que lida com densidades vari√°veis                                 | ‚Ä¢ Clusters de densidade vari√°vel<br>‚Ä¢ Oferece visualiza√ß√£o de acessibilidade<br>‚Ä¢ Robusto a par√¢metros<br>‚Ä¢ Detecta hierarquias                     | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Mais complexo que DBSCAN<br>‚Ä¢ Par√¢metro Œæ adicional<br>‚Ä¢ Implementa√ß√£o mais complexa                                                 | ‚Ä¢ An√°lise geoespacial<br>‚Ä¢ Telecomunica√ß√µes<br>‚Ä¢ Urbanismo<br>‚Ä¢ Reconhecimento de padr√µes                        | O(n¬≤)        |
| **Mean Shift**              | Localiza os m√°ximos da fun√ß√£o de densidade de pontos                                 | ‚Ä¢ N√£o param√©trico<br>‚Ä¢ N√£o requer n√∫mero de clusters<br>‚Ä¢ Robust a outliers<br>‚Ä¢ Encontra clusters de formato arbitr√°rio                            | ‚Ä¢ Computacionalmente caro<br>‚Ä¢ Dif√≠cil escolha da largura de banda<br>‚Ä¢ Converg√™ncia lenta<br>‚Ä¢ Problemas em alta dimensionalidade                                       | ‚Ä¢ Segmenta√ß√£o de imagens<br>‚Ä¢ Rastreamento de objetos<br>‚Ä¢ An√°lise de padr√µes<br>‚Ä¢ Computer vision               | O(n¬≤)        |
| **Affinity Propagation**    | Troca mensagens entre pontos para identificar "exemplares" de clusters               | ‚Ä¢ N√£o requer n√∫mero de clusters<br>‚Ä¢ Determina exemplares representativos<br>‚Ä¢ Descobre formas n√£o-esf√©ricas<br>‚Ä¢ Eficaz para pequenos datasets     | ‚Ä¢ Lento e intensivo em mem√≥ria<br>‚Ä¢ Dif√≠cil otimiza√ß√£o de par√¢metros<br>‚Ä¢ Sens√≠vel ao par√¢metro de prefer√™ncia<br>‚Ä¢ N√£o escala bem                                       | ‚Ä¢ Gen√¥mica<br>‚Ä¢ An√°lise de imagens<br>‚Ä¢ Agrupamento de faces<br>‚Ä¢ Redes de sensores                              | O(n¬≤t)       |
| **Gaussian Mixture Models** | Modela clusters como distribui√ß√µes gaussianas multivariadas                          | ‚Ä¢ Clusters probabil√≠sticos (soft clustering)<br>‚Ä¢ Flex√≠vel em formas de cluster<br>‚Ä¢ Medida de incerteza<br>‚Ä¢ Base estat√≠stica s√≥lida               | ‚Ä¢ Sens√≠vel √† inicializa√ß√£o<br>‚Ä¢ Pode convergir para m√≠nimos locais<br>‚Ä¢ Requer n√∫mero de clusters<br>‚Ä¢ Suposi√ß√£o de distribui√ß√£o normal                                  | ‚Ä¢ Processamento de √°udio<br>‚Ä¢ Modelagem de comportamento<br>‚Ä¢ Finan√ßas<br>‚Ä¢ Reconhecimento de imagens            | O(nkd¬≤)      |
| **Spectral Clustering**     | Usa autovalores da matriz de similaridade para reduzir dimens√µes antes do clustering | ‚Ä¢ Clusters de formas complexas<br>‚Ä¢ Fundamenta√ß√£o matem√°tica s√≥lida<br>‚Ä¢ N√£o faz suposi√ß√µes sobre forma<br>‚Ä¢ Robusto a outliers                     | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Sens√≠vel √† escolha do kernel<br>‚Ä¢ Constru√ß√£o do grafo impacta resultado<br>‚Ä¢ N√£o escala bem                                          | ‚Ä¢ Segmenta√ß√£o de imagens<br>‚Ä¢ An√°lise de redes<br>‚Ä¢ Bioinform√°tica<br>‚Ä¢ Processamento de fala                    | O(n¬≥)        |
| **BIRCH**                   | Clustering hier√°rquico para grandes datasets com √°rvores CF                          | ‚Ä¢ Escal√°vel para grandes datasets<br>‚Ä¢ Baixo uso de mem√≥ria<br>‚Ä¢ Tratamento de outliers<br>‚Ä¢ R√°pido em passagem √∫nica                               | ‚Ä¢ Sens√≠vel √† ordem dos dados<br>‚Ä¢ Limitado a clusters esf√©ricos<br>‚Ä¢ V√°rios par√¢metros para ajustar<br>‚Ä¢ Resultados dependem do limiar                                   | ‚Ä¢ Minera√ß√£o de dados<br>‚Ä¢ Streams de dados<br>‚Ä¢ An√°lise de logs<br>‚Ä¢ Grandes datasets                            | O(n)         |

## üîÑ Redu√ß√£o de Dimensionalidade

Esses algoritmos reduzem o n√∫mero de vari√°veis mantendo a maior quantidade de informa√ß√£o poss√≠vel.

| Algoritmo                                                | Descri√ß√£o                                                                 | Vantagens                                                                                                                                             | Desvantagens                                                                                                                             | Aplica√ß√µes                                                                                                   | Complexidade     |
| -------------------------------------------------------- | ------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------- |
| **PCA (Principal Component Analysis)**                   | Projeta dados em dire√ß√µes de m√°xima vari√¢ncia                             | ‚Ä¢ Preserva vari√¢ncia global<br>‚Ä¢ R√°pido e determin√≠stico<br>‚Ä¢ Sem par√¢metros para ajustar<br>‚Ä¢ Reduz multicolinearidade                               | ‚Ä¢ Limitado a transforma√ß√µes lineares<br>‚Ä¢ Sens√≠vel a escala<br>‚Ä¢ Componentes dif√≠ceis de interpretar<br>‚Ä¢ N√£o preserva dist√¢ncias locais | ‚Ä¢ Pr√©-processamento de dados<br>‚Ä¢ Visualiza√ß√£o<br>‚Ä¢ Compress√£o<br>‚Ä¢ Reconstru√ß√£o de imagens                  | O(nd¬≤)           |
| **t-SNE (t-Distributed Stochastic Neighbor Embedding)**  | Preserva similaridades locais em espa√ßo de baixa dimens√£o                 | ‚Ä¢ Excelente visualiza√ß√£o de clusters<br>‚Ä¢ Preserva estruturas locais<br>‚Ä¢ Lida com dados n√£o-lineares<br>‚Ä¢ Separa bem os clusters                     | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ N√£o determin√≠stico<br>‚Ä¢ Sens√≠vel a perplexity<br>‚Ä¢ N√£o apropriado para modelagem                     | ‚Ä¢ Visualiza√ß√£o de datasets<br>‚Ä¢ Explora√ß√£o de dados<br>‚Ä¢ An√°lise explorat√≥ria<br>‚Ä¢ Single-cell RNA-seq       | O(n¬≤)            |
| **LDA (Linear Discriminant Analysis)**                   | Projeta dados maximizando separabilidade entre classes                    | ‚Ä¢ √ìtimo para dados rotulados<br>‚Ä¢ Reduz dimensionalidade e classifica<br>‚Ä¢ Preserva informa√ß√£o discriminante<br>‚Ä¢ Robusto a pequenas amostras         | ‚Ä¢ Assume distribui√ß√£o normal<br>‚Ä¢ Limitado a transforma√ß√µes lineares<br>‚Ä¢ Limitado pelo n√∫mero de classes<br>‚Ä¢ Sens√≠vel a outliers       | ‚Ä¢ Reconhecimento facial<br>‚Ä¢ Classifica√ß√£o de texto<br>‚Ä¢ An√°lise de imagens<br>‚Ä¢ Marketing                   | O(nd¬≤)           |
| **UMAP (Uniform Manifold Approximation and Projection)** | T√©cnica n√£o-linear preservando tanto estrutura local quanto global        | ‚Ä¢ Mais r√°pido que t-SNE<br>‚Ä¢ Melhor preserva√ß√£o da estrutura global<br>‚Ä¢ Escal√°vel para grandes datasets<br>‚Ä¢ Base te√≥rica s√≥lida                     | ‚Ä¢ Sens√≠vel a hiperpar√¢metros<br>‚Ä¢ Menos intuitivo que PCA<br>‚Ä¢ N√£o determin√≠stico<br>‚Ä¢ Dif√≠cil interpreta√ß√£o                             | ‚Ä¢ Visualiza√ß√£o de dados<br>‚Ä¢ Single-cell analysis<br>‚Ä¢ An√°lise gen√¥mica<br>‚Ä¢ Processamento de imagens        | O(n log(n))      |
| **Autoencoder**                                          | Redes neurais que comprimem dados e depois os reconstroem                 | ‚Ä¢ Captura transforma√ß√µes n√£o-lineares<br>‚Ä¢ Adapt√°vel a diferentes tipos de dados<br>‚Ä¢ Aprende representa√ß√µes hier√°rquicas<br>‚Ä¢ Vers√°til em aplica√ß√µes | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Requer mais dados para treinar<br>‚Ä¢ Dif√≠cil otimiza√ß√£o<br>‚Ä¢ N√£o determin√≠stico                       | ‚Ä¢ Processamento de imagens<br>‚Ä¢ Detec√ß√£o de anomalias<br>‚Ä¢ Sistemas de recomenda√ß√£o<br>‚Ä¢ Compress√£o de dados | O(n √ó e √ó h √ó d) |
| **Factor Analysis**                                      | Modela vari√°veis observadas como combina√ß√µes lineares de fatores latentes | ‚Ä¢ Base estat√≠stica s√≥lida<br>‚Ä¢ Modela estrutura de covari√¢ncia<br>‚Ä¢ Interpreta fatores latentes<br>‚Ä¢ Estima erros de medi√ß√£o                          | ‚Ä¢ Assume linearidade<br>‚Ä¢ Sens√≠vel √† rota√ß√£o<br>‚Ä¢ Subjetivo na interpreta√ß√£o<br>‚Ä¢ Menos robusto que PCA                                  | ‚Ä¢ Psicometria<br>‚Ä¢ Ci√™ncias sociais<br>‚Ä¢ Marketing<br>‚Ä¢ Finan√ßas                                             | O(nd¬≤)           |
| **NMF (Non-negative Matrix Factorization)**              | Decomp√µe matrizes n√£o-negativas em fatores n√£o-negativos                  | ‚Ä¢ Resultados interpret√°veis<br>‚Ä¢ Decomposi√ß√£o aditiva (sem cancelamento)<br>‚Ä¢ Identifica partes dos dados<br>‚Ä¢ Bom para dados esparsos                | ‚Ä¢ Apenas para dados n√£o-negativos<br>‚Ä¢ N√£o √∫nico (m√∫ltiplas solu√ß√µes)<br>‚Ä¢ Sens√≠vel √† inicializa√ß√£o<br>‚Ä¢ Converg√™ncia lenta              | ‚Ä¢ Processamento de texto<br>‚Ä¢ Reconhecimento facial<br>‚Ä¢ √Åudio (separa√ß√£o de fontes)<br>‚Ä¢ Bioinform√°tica     | O(ndk)           |
| **Isomap**                                               | Preserva dist√¢ncias geod√©sicas em espa√ßo de baixa dimens√£o                | ‚Ä¢ Preserva estrutura global n√£o-linear<br>‚Ä¢ Captura manifolds<br>‚Ä¢ Base matem√°tica s√≥lida<br>‚Ä¢ Intuitivo (extens√£o de MDS)                            | ‚Ä¢ Sens√≠vel a ru√≠do<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Dif√≠cil escolher n¬∫ de vizinhos<br>‚Ä¢ Problemas com "buracos"                   | ‚Ä¢ An√°lise de imagens<br>‚Ä¢ Bioinform√°tica<br>‚Ä¢ Vis√£o computacional<br>‚Ä¢ Rob√≥tica                              | O(n¬≥)            |
| **Locally Linear Embedding (LLE)**                       | Preserva rela√ß√µes lineares locais em espa√ßo de baixa dimens√£o             | ‚Ä¢ Preserva estruturas locais<br>‚Ä¢ Eficiente computacionalmente<br>‚Ä¢ N√£o param√©trico<br>‚Ä¢ Captura manifolds n√£o-lineares                               | ‚Ä¢ Sens√≠vel ao n√∫mero de vizinhos<br>‚Ä¢ N√£o preserva estrutura global<br>‚Ä¢ Problemas com dados ruidosos<br>‚Ä¢ N√£o escala bem                | ‚Ä¢ An√°lise de voz<br>‚Ä¢ Processamento de imagens<br>‚Ä¢ Bioinform√°tica<br>‚Ä¢ An√°lise de movimento                 | O(dn¬≤)           |

## ü§ñ Aprendizado por Refor√ßo

Algoritmos onde um agente aprende a tomar a√ß√µes em um ambiente para maximizar uma recompensa cumulativa.

| Algoritmo                                     | Descri√ß√£o                                                             | Vantagens                                                                                                                                         | Desvantagens                                                                                                                           | Aplica√ß√µes                                                                                  | Complexidade            |
| --------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ----------------------- |
| **Q-Learning**                                | Aprende uma fun√ß√£o Q que mapeia estados e a√ß√µes para recompensas      | ‚Ä¢ N√£o requer modelo do ambiente<br>‚Ä¢ Converg√™ncia garantida<br>‚Ä¢ Trata espa√ßos discretos bem<br>‚Ä¢ Simples implementa√ß√£o                           | ‚Ä¢ Sofre com curse of dimensionality<br>‚Ä¢ Lento em problemas complexos<br>‚Ä¢ Limitado em espa√ßos cont√≠nuos<br>‚Ä¢ Explora√ß√£o vs explora√ß√£o | ‚Ä¢ Jogos de tabuleiro<br>‚Ä¢ Rob√≥tica simples<br>‚Ä¢ Sistemas de recomenda√ß√£o<br>‚Ä¢ Scheduling    | O(n_estados √ó n_a√ß√µes)  |
| **DQN (Deep Q-Network)**                      | Extens√£o do Q-Learning usando redes neurais para aproximar a fun√ß√£o Q | ‚Ä¢ Funciona em espa√ßos de estados grandes<br>‚Ä¢ Aprende representa√ß√µes complexas<br>‚Ä¢ Lida com entradas cont√≠nuas<br>‚Ä¢ Escal√°vel                    | ‚Ä¢ Treinamento inst√°vel<br>‚Ä¢ Hiperpar√¢metros sens√≠veis<br>‚Ä¢ Superestima√ß√£o de valores Q<br>‚Ä¢ Alto custo computacional                   | ‚Ä¢ Jogos de Atari<br>‚Ä¢ Rob√≥tica<br>‚Ä¢ Controle autom√°tico<br>‚Ä¢ Jogos complexos                | O(n √ó e √ó h √ó o)        |
| **Policy Gradient**                           | Otimiza diretamente a pol√≠tica de a√ß√µes via gradiente                 | ‚Ä¢ Lida naturalmente com a√ß√µes cont√≠nuas<br>‚Ä¢ Captura pol√≠ticas estoc√°sticas<br>‚Ä¢ Converg√™ncia mais est√°vel<br>‚Ä¢ Adequado para problemas complexos | ‚Ä¢ Alta vari√¢ncia nas estimativas<br>‚Ä¢ Pode convergir para √≥timos locais<br>‚Ä¢ Sens√≠vel a hiperpar√¢metros<br>‚Ä¢ Amostragem ineficiente    | ‚Ä¢ Controle rob√≥tico<br>‚Ä¢ Trading autom√°tico<br>‚Ä¢ Simula√ß√µes f√≠sicas<br>‚Ä¢ Jogos estrat√©gicos | Varia com implementa√ß√£o |
| **DDPG (Deep Deterministic Policy Gradient)** | Combina DQN e Policy Gradient para espa√ßos de a√ß√£o cont√≠nuos          | ‚Ä¢ Eficiente para a√ß√µes cont√≠nuas<br>‚Ä¢ Usa experi√™ncia passada<br>‚Ä¢ Aprendizado off-policy<br>‚Ä¢ Estabilidade melhorada                             | ‚Ä¢ Sens√≠vel a hiperpar√¢metros<br>‚Ä¢ Explora√ß√£o desafiadora<br>‚Ä¢ Treinamento complexo<br>‚Ä¢ Pode ser inst√°vel                              | ‚Ä¢ Controle de rob√¥s<br>‚Ä¢ Ve√≠culos aut√¥nomos<br>‚Ä¢ Sistemas f√≠sicos<br>‚Ä¢ Jogos cont√≠nuos      | O(n √ó e √ó h √ó o)        |
| **A3C (Asynchronous Advantage Actor-Critic)** | Treinamento ass√≠ncrono com m√∫ltiplos agentes em paralelo              | ‚Ä¢ Treinamento eficiente e est√°vel<br>‚Ä¢ Paraleliz√°vel<br>‚Ä¢ Melhor explora√ß√£o<br>‚Ä¢ Reduz correla√ß√£o entre amostras                                  | ‚Ä¢ Complexidade de implementa√ß√£o<br>‚Ä¢ Overhead de comunica√ß√£o<br>‚Ä¢ Sincroniza√ß√£o de par√¢metros<br>‚Ä¢ Ajuste de m√∫ltiplos hiperpar√¢metros | ‚Ä¢ Jogos 3D<br>‚Ä¢ Simula√ß√µes distribu√≠das<br>‚Ä¢ Controle rob√≥tico<br>‚Ä¢ Ambientes complexos     | Varia com implementa√ß√£o |
| **PPO (Proximal Policy Optimization)**        | Otimiza pol√≠tica limitando mudan√ßas muito grandes                     | ‚Ä¢ Estabilidade no treinamento<br>‚Ä¢ Simples implementa√ß√£o<br>‚Ä¢ Bom equil√≠brio explora√ß√£o/explora√ß√£o<br>‚Ä¢ Alta confiabilidade                       | ‚Ä¢ Menos eficiente em amostragem<br>‚Ä¢ Mais lento que alguns m√©todos<br>‚Ä¢ Hiperpar√¢metros sens√≠veis<br>‚Ä¢ Menos teoricamente fundamentado | ‚Ä¢ Rob√≥tica human√≥ide<br>‚Ä¢ Controle autom√°tico<br>‚Ä¢ OpenAI Gym<br>‚Ä¢ Simula√ß√µes f√≠sicas       | Varia com implementa√ß√£o |

## üåê Redes Neurais Avan√ßadas

Arquiteturas especializadas de redes neurais para tarefas espec√≠ficas.

| Algoritmo                                  | Descri√ß√£o                                                                              | Vantagens                                                                                                                                           | Desvantagens                                                                                                                                   | Aplica√ß√µes                                                                                                   | Complexidade            |
| ------------------------------------------ | -------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | ----------------------- |
| **CNN (Convolutional Neural Network)**     | Redes com camadas convolucionais para processamento de dados com estrutura espacial    | ‚Ä¢ Preserva rela√ß√µes espaciais<br>‚Ä¢ Redu√ß√£o de par√¢metros<br>‚Ä¢ Invari√¢ncia a transla√ß√µes<br>‚Ä¢ Transfer learning efetivo                              | ‚Ä¢ Alto custo computacional<br>‚Ä¢ Requer grandes volumes de dados<br>‚Ä¢ Complexidade de arquitetura<br>‚Ä¢ Dif√≠cil interpretabilidade               | ‚Ä¢ Reconhecimento de imagens<br>‚Ä¢ Vis√£o computacional<br>‚Ä¢ An√°lise de v√≠deo<br>‚Ä¢ Diagn√≥stico m√©dico           | O(n √ó k √ó f¬≤ √ó c¬≤)      |
| **RNN (Recurrent Neural Network)**         | Redes com conex√µes recorrentes para processar sequ√™ncias temporais                     | ‚Ä¢ Mem√≥ria de curto prazo<br>‚Ä¢ Compartilhamento de par√¢metros<br>‚Ä¢ Flex√≠vel em comprimento de entrada<br>‚Ä¢ Modela depend√™ncias temporais             | ‚Ä¢ Gradiente desvanecente<br>‚Ä¢ Treinamento lento<br>‚Ä¢ Dif√≠cil capturar depend√™ncias longas<br>‚Ä¢ Processamento sequencial                        | ‚Ä¢ Processamento de linguagem<br>‚Ä¢ Tradu√ß√£o autom√°tica<br>‚Ä¢ An√°lise de s√©ries temporais<br>‚Ä¢ Gera√ß√£o de texto | O(n √ó t √ó h¬≤)           |
| **LSTM (Long Short-Term Memory)**          | Variante de RNN com c√©lulas de mem√≥ria para capturar depend√™ncias de longo prazo       | ‚Ä¢ Captura depend√™ncias longas<br>‚Ä¢ Resolve problema do gradiente desvanecente<br>‚Ä¢ Controle de fluxo de informa√ß√£o<br>‚Ä¢ Estabilidade no treinamento | ‚Ä¢ Mais par√¢metros que RNNs simples<br>‚Ä¢ Complexidade computacional<br>‚Ä¢ Dif√≠cil interpreta√ß√£o<br>‚Ä¢ Treinamento demorado                        | ‚Ä¢ Tradu√ß√£o autom√°tica<br>‚Ä¢ Reconhecimento de fala<br>‚Ä¢ Gera√ß√£o de m√∫sica<br>‚Ä¢ Previs√£o financeira            | O(n √ó t √ó h¬≤)           |
| **GRU (Gated Recurrent Unit)**             | Simplifica√ß√£o do LSTM com menos portas e par√¢metros                                    | ‚Ä¢ Mais simples que LSTM<br>‚Ä¢ Treinamento mais r√°pido<br>‚Ä¢ Menos par√¢metros<br>‚Ä¢ Performance compar√°vel                                              | ‚Ä¢ Menor capacidade expressiva<br>‚Ä¢ Menos controle sobre mem√≥ria<br>‚Ä¢ Menos estudado teoricamente<br>‚Ä¢ Menos flex√≠vel que LSTM                  | ‚Ä¢ Processamento de linguagem<br>‚Ä¢ An√°lise de sentimento<br>‚Ä¢ Chatbots<br>‚Ä¢ Reconhecimento de a√ß√µes           | O(n √ó t √ó h¬≤)           |
| **Transformer**                            | Arquitetura baseada em mecanismos de aten√ß√£o para processamento paralelo de sequ√™ncias | ‚Ä¢ Paraleliza√ß√£o eficiente<br>‚Ä¢ Captura depend√™ncias de longo alcance<br>‚Ä¢ Alta escalabilidade<br>‚Ä¢ State-of-the-art em NLP                          | ‚Ä¢ Alto consumo de mem√≥ria<br>‚Ä¢ Complexidade quadr√°tica com tamanho<br>‚Ä¢ Muitos hiperpar√¢metros<br>‚Ä¢ Requer grandes datasets                    | ‚Ä¢ Processamento de linguagem<br>‚Ä¢ Tradu√ß√£o neural<br>‚Ä¢ Modelos generativos<br>‚Ä¢ Compreens√£o de texto         | O(n √ó t¬≤ √ó d)           |
| **GANs (Generative Adversarial Networks)** | Duas redes competindo: gerador cria dados e discriminador os avalia                    | ‚Ä¢ Gera dados sint√©ticos realistas<br>‚Ä¢ Aprendizado n√£o supervisionado<br>‚Ä¢ Alta qualidade de sa√≠da<br>‚Ä¢ Captura distribui√ß√µes complexas             | ‚Ä¢ Treinamento inst√°vel<br>‚Ä¢ Mode collapse<br>‚Ä¢ Dif√≠cil converg√™ncia<br>‚Ä¢ M√©tricas de avalia√ß√£o subjetivas                                      | ‚Ä¢ Gera√ß√£o de imagens<br>‚Ä¢ Super-resolu√ß√£o<br>‚Ä¢ Transfer√™ncia de estilo<br>‚Ä¢ S√≠ntese de dados                 | Varia com implementa√ß√£o |
| **VAE (Variational Autoencoder)**          | Autoencoder que aprende distribui√ß√£o probabil√≠stica latente                            | ‚Ä¢ Modelagem generativa<br>‚Ä¢ Espa√ßo latente cont√≠nuo<br>‚Ä¢ Base probabil√≠stica s√≥lida<br>‚Ä¢ Regulariza√ß√£o integrada                                    | ‚Ä¢ Imagens geradas menos n√≠tidas<br>‚Ä¢ Dificuldade com detalhes finos<br>‚Ä¢ Equil√≠brio dif√≠cil entre termos de perda<br>‚Ä¢ Complexidade matem√°tica | ‚Ä¢ Gera√ß√£o de imagens<br>‚Ä¢ Compress√£o sem√¢ntica<br>‚Ä¢ Detec√ß√£o de anomalias<br>‚Ä¢ Preenchimento de dados        | O(n √ó e √ó h √ó d)        |
| **Graph Neural Networks**                  | Redes para processamento de dados estruturados em grafos                               | ‚Ä¢ Preserva estrutura relacional<br>‚Ä¢ Invari√¢ncia a permuta√ß√µes<br>‚Ä¢ Transfer√≠vel entre grafos<br>‚Ä¢ Combina informa√ß√£o local e global                | ‚Ä¢ Alta complexidade computacional<br>‚Ä¢ Desafios de escala<br>‚Ä¢ Representa√ß√£o de atributos<br>‚Ä¢ Treinamento complexo                            | ‚Ä¢ Redes sociais<br>‚Ä¢ Qu√≠mica molecular<br>‚Ä¢ Sistemas de recomenda√ß√£o<br>‚Ä¢ Detec√ß√£o de fraudes                | O(n √ó e √ó m)            |
| **Diffusion Models**                       | Redes que aprendem a reverter um processo de difus√£o que adiciona ru√≠do aos dados      | ‚Ä¢ Gera√ß√£o de alta qualidade<br>‚Ä¢ Treinamento est√°vel<br>‚Ä¢ Controle preciso<br>‚Ä¢ Base te√≥rica s√≥lida                                                 | ‚Ä¢ Gera√ß√£o lenta<br>‚Ä¢ Alto custo computacional<br>‚Ä¢ Complexidade matem√°tica<br>‚Ä¢ Muitos passos de infer√™ncia                                    | ‚Ä¢ Gera√ß√£o de imagens<br>‚Ä¢ S√≠ntese de √°udio<br>‚Ä¢ Restaura√ß√£o<br>‚Ä¢ Design molecular                            | O(n √ó d √ó t)            |

## üß¨ M√©todos Ensemble e Meta-Aprendizado

T√©cnicas que combinam m√∫ltiplos modelos para melhorar performance e robustez.

| Algoritmo                           | Descri√ß√£o                                                                            | Vantagens                                                                                                                                        | Desvantagens                                                                                                                  | Aplica√ß√µes                                                                                              | Complexidade                                     |
| ----------------------------------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| **Bagging (Bootstrap Aggregating)** | Treina modelos em subconjuntos aleat√≥rios com substitui√ß√£o e combina resultados      | ‚Ä¢ Reduz vari√¢ncia<br>‚Ä¢ Evita overfitting<br>‚Ä¢ Paraleliz√°vel<br>‚Ä¢ Estabiliza resultados                                                           | ‚Ä¢ N√£o reduz vi√©s<br>‚Ä¢ Maior complexidade computacional<br>‚Ä¢ Menos interpret√°vel<br>‚Ä¢ Requer mais mem√≥ria                      | ‚Ä¢ Random Forests<br>‚Ä¢ Robustez em previs√µes<br>‚Ä¢ Redu√ß√£o de ru√≠do<br>‚Ä¢ Sistemas cr√≠ticos                | O(n_models √ó model_complexity)                   |
| **Boosting**                        | Treina modelos sequencialmente, focando em erros dos anteriores                      | ‚Ä¢ Alta precis√£o<br>‚Ä¢ Reduz vi√©s e vari√¢ncia<br>‚Ä¢ Funciona bem com dados desbalanceados<br>‚Ä¢ Automaticamente destaca exemplos dif√≠ceis            | ‚Ä¢ Propenso a overfitting<br>‚Ä¢ Sens√≠vel a outliers<br>‚Ä¢ Sequencial (n√£o paraleliz√°vel)<br>‚Ä¢ Complexidade de implementa√ß√£o      | ‚Ä¢ Competi√ß√µes de ML<br>‚Ä¢ Sistemas de alta precis√£o<br>‚Ä¢ Detec√ß√£o de fraudes<br>‚Ä¢ Classifica√ß√£o avan√ßada | O(n_models √ó model_complexity)                   |
| **Stacking**                        | Combina previs√µes de diversos modelos usando outro modelo (meta-learner)             | ‚Ä¢ Aproveita pontos fortes de diferentes modelos<br>‚Ä¢ Alta performance<br>‚Ä¢ Captura diferentes padr√µes<br>‚Ä¢ Robustez a varia√ß√µes de dados         | ‚Ä¢ Complexidade de implementa√ß√£o<br>‚Ä¢ Risco de overfitting<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Dificuldade em interpreta√ß√£o | ‚Ä¢ Competi√ß√µes Kaggle<br>‚Ä¢ Sistemas cr√≠ticos<br>‚Ä¢ Previs√µes robustas<br>‚Ä¢ Sistemas h√≠bridos              | O(n_models √ó model_complexity + meta_complexity) |
| **Voting**                          | Combina previs√µes de m√∫ltiplos modelos por voto ou m√©dia                             | ‚Ä¢ Implementa√ß√£o simples<br>‚Ä¢ Reduz vari√¢ncia<br>‚Ä¢ Robusto a outliers<br>‚Ä¢ F√°cil paraleliza√ß√£o                                                    | ‚Ä¢ Efici√™ncia limitada<br>‚Ä¢ Redund√¢ncia poss√≠vel<br>‚Ä¢ N√£o aprende pesos otimizados<br>‚Ä¢ Desbalanceamento de performance        | ‚Ä¢ Classifica√ß√µes robustas<br>‚Ä¢ Sistemas de recomenda√ß√£o<br>‚Ä¢ Comit√™s de decis√£o<br>‚Ä¢ Previs√µes cr√≠ticas | O(n_models √ó model_complexity)                   |
| **Feature Selection Ensemble**      | Combina modelos treinados em diferentes subconjuntos de atributos                    | ‚Ä¢ Reduz dimensionalidade<br>‚Ä¢ Melhora generaliza√ß√£o<br>‚Ä¢ Destaca atributos importantes<br>‚Ä¢ Reduz overfitting                                    | ‚Ä¢ Escolha de subconjuntos<br>‚Ä¢ Risco de perda de informa√ß√£o<br>‚Ä¢ Complexidade adicional<br>‚Ä¢ Potencial redund√¢ncia            | ‚Ä¢ Dados de alta dimens√£o<br>‚Ä¢ Gen√¥mica<br>‚Ä¢ Processamento de imagens<br>‚Ä¢ Bioinform√°tica                | O(n_subsets √ó model_complexity)                  |
| **Meta-Learning**                   | Aprende como combinar modelos ou escolher o melhor algoritmo para determinados dados | ‚Ä¢ Otimiza√ß√£o autom√°tica<br>‚Ä¢ Adaptabilidade a diferentes problemas<br>‚Ä¢ Aproveitamento de experi√™ncia passada<br>‚Ä¢ Transfer√™ncia de conhecimento | ‚Ä¢ Complexidade te√≥rica<br>‚Ä¢ Requer muitos meta-dados<br>‚Ä¢ Alto custo computacional<br>‚Ä¢ Dif√≠cil interpreta√ß√£o                 | ‚Ä¢ AutoML<br>‚Ä¢ Sistemas adaptativos<br>‚Ä¢ Otimiza√ß√£o de hiperpar√¢metros<br>‚Ä¢ Transfer√™ncia de dom√≠nio     | Varia significativamente                         |

## üîé Aprendizado Semissupervisionado e Auto-Supervisionado

T√©cnicas que utilizam dados n√£o rotulados ou criam supervis√£o a partir dos pr√≥prios dados.

| Algoritmo                    | Descri√ß√£o                                                                           | Vantagens                                                                                                                                   | Desvantagens                                                                                                                             | Aplica√ß√µes                                                                                                              | Complexidade                      |
| ---------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | --------------------------------- |
| **Label Propagation**        | Propaga r√≥tulos conhecidos atrav√©s do grafo de similaridade                         | ‚Ä¢ Aproveita estrutura dos dados<br>‚Ä¢ Usa poucos dados rotulados<br>‚Ä¢ Base matem√°tica s√≥lida<br>‚Ä¢ Implementa√ß√£o simples                      | ‚Ä¢ Sens√≠vel √† constru√ß√£o do grafo<br>‚Ä¢ Escalabilidade limitada<br>‚Ä¢ Suposi√ß√£o de suavidade<br>‚Ä¢ Problemas com dados ruidosos              | ‚Ä¢ Classifica√ß√£o de texto<br>‚Ä¢ Detec√ß√£o de comunidades<br>‚Ä¢ An√°lise de redes sociais<br>‚Ä¢ Bioinform√°tica                 | O(n¬≤i)                            |
| **Self-Training**            | Modelo treina com seus pr√≥prios r√≥tulos de alta confian√ßa                           | ‚Ä¢ Implementa√ß√£o simples<br>‚Ä¢ Adapt√°vel a qualquer modelo<br>‚Ä¢ Escal√°vel<br>‚Ä¢ Iterativo                                                      | ‚Ä¢ Refor√ßo de erros<br>‚Ä¢ Vi√©s de confirma√ß√£o<br>‚Ä¢ Sens√≠vel ao modelo base<br>‚Ä¢ Determina√ß√£o de confian√ßa                                  | ‚Ä¢ Reconhecimento de fala<br>‚Ä¢ Classifica√ß√£o de texto<br>‚Ä¢ Detec√ß√£o de objetos<br>‚Ä¢ Tradu√ß√£o autom√°tica                  | O(model_complexity √ó i)           |
| **Co-Training**              | Treina m√∫ltiplos modelos em diferentes "vis√µes" dos dados                           | ‚Ä¢ Usa m√∫ltiplas perspectivas<br>‚Ä¢ Reduz overfitting<br>‚Ä¢ Aproveita informa√ß√£o complementar<br>‚Ä¢ Robusto a ru√≠do                             | ‚Ä¢ Requer vis√µes independentes<br>‚Ä¢ Complexidade de implementa√ß√£o<br>‚Ä¢ Dificuldade com vis√µes correlacionadas<br>‚Ä¢ Coordena√ß√£o de modelos | ‚Ä¢ Classifica√ß√£o multimodal<br>‚Ä¢ Processamento de texto/imagem<br>‚Ä¢ An√°lise de sentimento<br>‚Ä¢ Reconhecimento de objetos | O(n_views √ó model_complexity √ó i) |
| **Contrastive Learning**     | Aprende representa√ß√µes aproximando exemplos similares e afastando diferentes        | ‚Ä¢ N√£o requer r√≥tulos<br>‚Ä¢ Representa√ß√µes robustas<br>‚Ä¢ Transferibilidade<br>‚Ä¢ Estado da arte em vis√£o computacional                         | ‚Ä¢ Defini√ß√£o de pares contrastivos<br>‚Ä¢ Alto custo computacional<br>‚Ä¢ Sens√≠vel a hiperpar√¢metros<br>‚Ä¢ Necessita de augmentations          | ‚Ä¢ Vis√£o computacional<br>‚Ä¢ NLP<br>‚Ä¢ Reconhecimento de √°udio<br>‚Ä¢ Transfer learning                                      | O(batch_size¬≤ √ó d)                |
| **Masked Language Modeling** | Prediz tokens mascarados em sequ√™ncias para aprender representa√ß√µes                 | ‚Ä¢ Aprendizado contextual<br>‚Ä¢ N√£o requer r√≥tulos<br>‚Ä¢ Escal√°vel<br>‚Ä¢ Captura rela√ß√µes sem√¢nticas                                            | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Requer grandes corpora<br>‚Ä¢ Vi√©s de mascaramento<br>‚Ä¢ Treinamento longo                              | ‚Ä¢ BERT e derivados<br>‚Ä¢ Representa√ß√£o de texto<br>‚Ä¢ Compreens√£o de linguagem<br>‚Ä¢ Transfer learning em NLP              | O(n √ó t √ó h¬≤)                     |
| **SimCLR**                   | Framework auto-supervisionado com augmentations e contrastive loss                  | ‚Ä¢ Representa√ß√µes robustas<br>‚Ä¢ N√£o requer dados rotulados<br>‚Ä¢ Estado da arte em vis√£o<br>‚Ä¢ Simplicidade conceitual                         | ‚Ä¢ Grandes batches necess√°rios<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Dependente de augmentations<br>‚Ä¢ Requer ajuste fino                 | ‚Ä¢ Classifica√ß√£o de imagens<br>‚Ä¢ Detec√ß√£o de objetos<br>‚Ä¢ Segmenta√ß√£o<br>‚Ä¢ Transfer learning                             | O(batch_size¬≤ √ó d)                |
| **FixMatch**                 | Combina consistency training e pseudo-labeling para aprendizado semissupervisionado | ‚Ä¢ Alta performance com poucos r√≥tulos<br>‚Ä¢ Simplicidade conceitual<br>‚Ä¢ Efici√™ncia computacional<br>‚Ä¢ Estado da arte em semissupervisionado | ‚Ä¢ Sens√≠vel a thresholds<br>‚Ä¢ Necessita de augmentations fortes<br>‚Ä¢ Dependente do modelo base<br>‚Ä¢ Equil√≠brio entre componentes          | ‚Ä¢ Classifica√ß√£o de imagens<br>‚Ä¢ Reconhecimento de objetos<br>‚Ä¢ Dados m√©dicos limitados<br>‚Ä¢ Dados com r√≥tulos escassos  | O(model_complexity)               |

## üõ†Ô∏è T√©cnicas de Otimiza√ß√£o e Regulariza√ß√£o

M√©todos para melhorar treinamento, converg√™ncia e generaliza√ß√£o de modelos.

| Algoritmo                             | Descri√ß√£o                                                                 | Vantagens                                                                                                                                    | Desvantagens                                                                                                                                      | Aplica√ß√µes                                                                                                                       | Complexidade      |
| ------------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ----------------- |
| **Gradient Descent**                  | Otimiza√ß√£o iterativa minimizando fun√ß√£o de perda via gradiente            | ‚Ä¢ Intuitivo e fundamental<br>‚Ä¢ Base para maioria dos algoritmos<br>‚Ä¢ Implementa√ß√£o simples<br>‚Ä¢ Converg√™ncia garantida para fun√ß√µes convexas | ‚Ä¢ Lento em grandes datasets<br>‚Ä¢ Pode ficar preso em m√≠nimos locais<br>‚Ä¢ Sens√≠vel √† taxa de aprendizado<br>‚Ä¢ Ineficiente para gradientes esparsos | ‚Ä¢ Base para maioria dos algoritmos de ML<br>‚Ä¢ Otimiza√ß√£o de modelos<br>‚Ä¢ Calibra√ß√£o de par√¢metros<br>‚Ä¢ Regress√£o e classifica√ß√£o | O(nd)             |
| **SGD (Stochastic Gradient Descent)** | Vers√£o estoc√°stica que usa amostras aleat√≥rias                            | ‚Ä¢ R√°pido e eficiente<br>‚Ä¢ Escape de m√≠nimos locais<br>‚Ä¢ Menor uso de mem√≥ria<br>‚Ä¢ √ötil para grandes datasets                                 | ‚Ä¢ Converg√™ncia ruidosa<br>‚Ä¢ Ajuste fino de learning rate<br>‚Ä¢ Pode n√£o convergir exatamente<br>‚Ä¢ Menos determin√≠stico                             | ‚Ä¢ Deep learning<br>‚Ä¢ Grandes datasets<br>‚Ä¢ Aprendizado online<br>‚Ä¢ Modelos complexos                                             | O(d) por itera√ß√£o |
| **Adam**                              | Otimizador adaptativo com momentum e learning rates individuais           | ‚Ä¢ Adaptativo por par√¢metro<br>‚Ä¢ Converg√™ncia r√°pida<br>‚Ä¢ Robusto a hiperpar√¢metros<br>‚Ä¢ Lida bem com gradientes esparsos                     | ‚Ä¢ Maior uso de mem√≥ria<br>‚Ä¢ Generaliza√ß√£o √†s vezes pior que SGD<br>‚Ä¢ Sens√≠vel a inicializa√ß√£o<br>‚Ä¢ Hiperpar√¢metros adicionais                     | ‚Ä¢ Deep learning<br>‚Ä¢ Redes neurais complexas<br>‚Ä¢ NLP<br>‚Ä¢ Vis√£o computacional                                                   | O(d)              |
| **Dropout**                           | Desativa aleatoriamente neur√¥nios durante treinamento                     | ‚Ä¢ Previne overfitting<br>‚Ä¢ Atua como ensemble impl√≠cito<br>‚Ä¢ Implementa√ß√£o simples<br>‚Ä¢ For√ßa redund√¢ncia robusta                            | ‚Ä¢ Requer mais √©pocas<br>‚Ä¢ Aumenta tempo de treinamento<br>‚Ä¢ Dif√≠cil calibrar probabilidade<br>‚Ä¢ Afeta modelos pequenos                            | ‚Ä¢ Redes neurais profundas<br>‚Ä¢ Modelos propensos a overfitting<br>‚Ä¢ Transfer learning<br>‚Ä¢ Grandes arquiteturas                  | O(1)              |
| **Batch Normalization**               | Normaliza ativa√ß√µes por batch para estabilizar treinamento                | ‚Ä¢ Treinamento mais r√°pido<br>‚Ä¢ Reduz sensibilidade a inicializa√ß√£o<br>‚Ä¢ Atua como regulariza√ß√£o<br>‚Ä¢ Permite maiores learning rates          | ‚Ä¢ Comportamento diferente em infer√™ncia<br>‚Ä¢ Depend√™ncia do tamanho do batch<br>‚Ä¢ Overhead computacional<br>‚Ä¢ Requer ajustes para RNNs            | ‚Ä¢ Redes neurais profundas<br>‚Ä¢ CNNs<br>‚Ä¢ Acelera√ß√£o de treinamento<br>‚Ä¢ Modelos inst√°veis                                        | O(nd)             |
| **Early Stopping**                    | Interrompe treinamento quando performance em valida√ß√£o piora              | ‚Ä¢ Simples e efetivo<br>‚Ä¢ N√£o requer par√¢metros adicionais<br>‚Ä¢ Previne overfitting<br>‚Ä¢ Economiza tempo                                      | ‚Ä¢ Requer conjunto de valida√ß√£o<br>‚Ä¢ Crit√©rio de parada subjetivo<br>‚Ä¢ Pode parar prematuramente<br>‚Ä¢ Pode n√£o convergir adequadamente             | ‚Ä¢ Qualquer modelo iterativo<br>‚Ä¢ Deep learning<br>‚Ä¢ Competi√ß√µes de ML<br>‚Ä¢ Modelos complexos                                     | O(1)              |
| **L1/L2 Regularization**              | Adiciona penalidade √† fun√ß√£o de perda baseado na magnitude dos par√¢metros | ‚Ä¢ Previne overfitting<br>‚Ä¢ L1: sele√ß√£o de atributos<br>‚Ä¢ L2: coeficientes menores<br>‚Ä¢ Implementa√ß√£o simples                                 | ‚Ä¢ Requer ajuste de for√ßa<br>‚Ä¢ Pode prejudicar aprendizado<br>‚Ä¢ N√£o ideal para todos modelos<br>‚Ä¢ Efeito varia entre problemas                     | ‚Ä¢ Regress√£o Linear<br>‚Ä¢ Redes neurais<br>‚Ä¢ SVM<br>‚Ä¢ Modelos lineares                                                             | O(d)              |

## üìâ Detec√ß√£o de Anomalias e Aprendizado com Dados Desbalanceados

T√©cnicas especializadas para identificar outliers e lidar com classes desproporcionais.

| Algoritmo                                              | Descri√ß√£o                                                   | Vantagens                                                                                                                             | Desvantagens                                                                                                                            | Aplica√ß√µes                                                                                                             | Complexidade       |
| ------------------------------------------------------ | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------ |
| **Isolation Forest**                                   | Isola anomalias construindo parti√ß√µes aleat√≥rias recursivas | ‚Ä¢ Eficiente para grandes datasets<br>‚Ä¢ Eficaz para alta dimensionalidade<br>‚Ä¢ N√£o requer densidade/dist√¢ncia<br>‚Ä¢ Baixa complexidade  | ‚Ä¢ Sens√≠vel a ru√≠do<br>‚Ä¢ Dificuldade com anomalias locais<br>‚Ä¢ Requer ajuste de hiperpar√¢metros<br>‚Ä¢ Pode falhar em dados complexos      | ‚Ä¢ Detec√ß√£o de fraudes<br>‚Ä¢ Monitoramento de sistemas<br>‚Ä¢ Ciberseguran√ßa<br>‚Ä¢ Controle de qualidade                    | O(n log(n))        |
| **One-Class SVM**                                      | SVM que aprende fronteira ao redor dos dados normais        | ‚Ä¢ Eficaz em alta dimensionalidade<br>‚Ä¢ Base matem√°tica s√≥lida<br>‚Ä¢ Flex√≠vel via kernels<br>‚Ä¢ N√£o assume distribui√ß√£o                  | ‚Ä¢ Sens√≠vel a hiperpar√¢metros<br>‚Ä¢ Dif√≠cil interpreta√ß√£o<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Sens√≠vel a outliers no treino            | ‚Ä¢ Detec√ß√£o de intrus√£o<br>‚Ä¢ Diagn√≥stico de falhas<br>‚Ä¢ Monitoramento de equipamentos<br>‚Ä¢ Detec√ß√£o de novidades        | O(n¬≥)              |
| **Local Outlier Factor (LOF)**                         | Detecta anomalias comparando densidade local de pontos      | ‚Ä¢ Detecta anomalias locais<br>‚Ä¢ N√£o assume distribui√ß√£o<br>‚Ä¢ Adapt√°vel a diferentes densidades<br>‚Ä¢ Scores interpret√°veis             | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Sens√≠vel a escolha de k<br>‚Ä¢ Dificuldade em alta dimensionalidade<br>‚Ä¢ Defini√ß√£o de threshold       | ‚Ä¢ Detec√ß√£o de fraudes<br>‚Ä¢ Monitoramento industrial<br>‚Ä¢ An√°lise de tr√°fego de rede<br>‚Ä¢ IoT e sensores                | O(n¬≤)              |
| **Autoencoders para Anomalias**                        | Identifica anomalias pelo erro de reconstru√ß√£o              | ‚Ä¢ Captura padr√µes complexos<br>‚Ä¢ Adapt√°vel a diferentes dados<br>‚Ä¢ Lida com alta dimensionalidade<br>‚Ä¢ Aprendizado n√£o supervisionado | ‚Ä¢ Requer muitos dados normais<br>‚Ä¢ Dif√≠cil ajuste de arquitetura<br>‚Ä¢ Threshold de erro subjetivo<br>‚Ä¢ Treinamento complexo             | ‚Ä¢ Detec√ß√£o de falhas<br>‚Ä¢ Ciberseguran√ßa<br>‚Ä¢ Diagn√≥stico m√©dico<br>‚Ä¢ Monitoramento industrial                         | O(n √ó e √ó h √ó d)   |
| **SMOTE (Synthetic Minority Over-sampling Technique)** | Gera exemplos sint√©ticos para classe minorit√°ria            | ‚Ä¢ Melhora balanceamento<br>‚Ä¢ Reduz overfitting<br>‚Ä¢ Implementa√ß√£o simples<br>‚Ä¢ Preserve vari√¢ncia intraclasse                         | ‚Ä¢ Pode gerar exemplos inv√°lidos<br>‚Ä¢ Intensifica ru√≠do<br>‚Ä¢ Sens√≠vel a outliers<br>‚Ä¢ Problemas em alta dimensionalidade                 | ‚Ä¢ Fraudes financeiras<br>‚Ä¢ Diagn√≥stico m√©dico<br>‚Ä¢ Detec√ß√£o de falhas raras<br>‚Ä¢ Ecologia (esp√©cies raras)             | O(n_minor¬≤)        |
| **ADASYN (Adaptive Synthetic Sampling)**               | Extens√£o do SMOTE que foca em exemplos dif√≠ceis             | ‚Ä¢ Adaptativo √† complexidade local<br>‚Ä¢ Prioriza √°reas dif√≠ceis<br>‚Ä¢ Mais eficaz que SMOTE<br>‚Ä¢ Balanceamento proporcional             | ‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ Sens√≠vel a ru√≠do<br>‚Ä¢ Complexidade adicional<br>‚Ä¢ Par√¢metros adicionais                             | ‚Ä¢ Diagn√≥stico m√©dico<br>‚Ä¢ Detec√ß√£o de eventos raros<br>‚Ä¢ √Åreas com fronteiras complexas<br>‚Ä¢ Reconhecimento de padr√µes | O(n_minor¬≤)        |
| **Tomek Links**                                        | Remove pares de exemplos pr√≥ximos de classes opostas        | ‚Ä¢ Clareia fronteiras de decis√£o<br>‚Ä¢ Remove ru√≠do e sobreposi√ß√£o<br>‚Ä¢ Melhora classifica√ß√£o<br>‚Ä¢ Complementa oversampling             | ‚Ä¢ Reduz dados de treinamento<br>‚Ä¢ Sens√≠vel √† m√©trica de dist√¢ncia<br>‚Ä¢ Computacionalmente intensivo<br>‚Ä¢ N√£o resolve desbalanceamento   | ‚Ä¢ Pr√©-processamento<br>‚Ä¢ Limpeza de dados<br>‚Ä¢ Combinado com oversampling<br>‚Ä¢ Melhorar fronteiras                     | O(n¬≤)              |
| **Cluster-Based Undersampling**                        | Agrupa a classe majorit√°ria e seleciona representantes      | ‚Ä¢ Preserva distribui√ß√£o original<br>‚Ä¢ Mant√©m vari√¢ncia<br>‚Ä¢ Reduz redund√¢ncia<br>‚Ä¢ Informativo e eficiente                            | ‚Ä¢ Dependente da qualidade do clustering<br>‚Ä¢ Sens√≠vel ao algoritmo de cluster<br>‚Ä¢ Escolha de representantes<br>‚Ä¢ Par√¢metros adicionais | ‚Ä¢ Dados textuais<br>‚Ä¢ Imagens<br>‚Ä¢ Reconhecimento de padr√µes<br>‚Ä¢ Problemas multiclasse                                | O(n_major √ó k √ó i) |

## üåü Legendas e Nota√ß√µes de Complexidade

- **n**: N√∫mero de amostras
- **d**: N√∫mero de atributos/dimens√µes
- **k**: N√∫mero de clusters ou vizinhos
- **t**: Comprimento da sequ√™ncia temporal
- **i**: N√∫mero de itera√ß√µes
- **h**: Tamanho da camada oculta
- **e**: N√∫mero de √©pocas
- **o**: Tamanho da sa√≠da
- **f**: Tamanho do filtro em CNNs
- **c**: N√∫mero de canais
- **n_trees**: N√∫mero de √°rvores
- **n_leaves**: N√∫mero de folhas
- **n_states**: N√∫mero de estados
- **n_actions**: N√∫mero de a√ß√µes
- **n_minor**: N√∫mero de exemplos da classe minorit√°ria
- **n_major**: N√∫mero de exemplos da classe majorit√°ria

## üîó Considera√ß√µes Adicionais

### Sele√ß√£o de Algoritmos

Escolher o algoritmo correto depende de v√°rios fatores:

- **Natureza dos dados**: tamanho, dimensionalidade, ru√≠do, balanceamento
- **Tipo de problema**: classifica√ß√£o, regress√£o, clustering, etc.
- **Restri√ß√µes**: velocidade, interpretabilidade, precis√£o, mem√≥ria
- **Conhecimento do dom√≠nio**: estrutura conhecida dos dados

### Avalia√ß√£o de Modelos

M√©tricas comuns para avalia√ß√£o:

- **Classifica√ß√£o**: acur√°cia, precis√£o, recall, F1-score, AUC-ROC, log loss
- **Regress√£o**: MSE, RMSE, MAE, R¬≤, MAPE
- **Clustering**: silhouette score, √≠ndice Davies-Bouldin, √≠ndice Calinski-Harabasz
- **Ranking**: NDCG, MAP, MRR

### Considera√ß√µes Pr√°ticas

- **Pr√©-processamento**: normaliza√ß√£o, imputa√ß√£o, codifica√ß√£o
- **Valida√ß√£o cruzada**: k-fold, stratified, leave-one-out, temporal
- **Ajuste de hiperpar√¢metros**: grid search, random search, Bayesian optimization
- **Engenharia de features**: sele√ß√£o, extra√ß√£o, transforma√ß√£o
- **Interpretabilidade**: SHAP values, LIME, feature importance, partial dependence plots

### Tend√™ncias Emergentes

- **Aprendizado federado**: treinamento distribu√≠do preservando privacidade
- **Modelos fundacionais**: modelos pr√©-treinados em grande escala
- **Neural Architecture Search (NAS)**: arquiteturas de redes neurais automatizadas
- **Machine Learning Systems Design**: MLOps, monitoramento, automa√ß√£o
- **Aprendizado por refor√ßo multi-agentes**: sistemas colaborativos de RL
- **Machine Learning economizador de dados**: aprendizado com poucos exemplos

### Ferramentas e Bibliotecas

- **Python**: scikit-learn, TensorFlow, PyTorch, Keras, XGBoost, LightGBM
- **R**: caret, mlr, randomForest, glmnet
- **Big Data**: Spark MLlib, H2O, Dask-ML
- **AutoML**: Auto-Sklearn, TPOT, AutoKeras, H2O AutoML
- **Visualiza√ß√£o**: Matplotlib, Seaborn, Plotly, Yellowbrick
